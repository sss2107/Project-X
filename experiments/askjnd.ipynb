{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20066f6c-73a7-4d29-b53e-38106cef891b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-12 19:09:46.909 | INFO     | __main__:<module>:348 - Start of predictor\n",
      "2023-08-12 19:09:46.910 | INFO     | __main__:<module>:349 - Creating the chatbot database using previously saved document store\n",
      "2023-08-12 19:09:46.911 | INFO     | __main__:<module>:351 - Creating the chatbot object\n",
      "2023-08-12 19:09:46.912 Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "2023-08-12 19:09:47.709 Use pytorch device: cpu\n",
      "2023-08-12 19:09:47.709 | INFO     | __main__:__init__:102 - Embedding model = sentence-transformers/all-mpnet-base-v2\n",
      "2023-08-12 19:09:47.710 | INFO     | __main__:__init__:104 - Reading in vectorstore DB from training script\n",
      "2023-08-12 19:09:47.724 | INFO     | __main__:transformation:369 - ********************\n",
      "2023-08-12 19:09:47.725 | INFO     | __main__:transformation:370 - Start /invocations\n",
      "2023-08-12 19:09:47.725 | INFO     | __main__:transformation:389 - Query: How many paid annual leave I have?\n",
      "Query length: 7\n",
      "2023-08-12 19:09:47.726 | INFO     | __main__:transformation:409 - Use Generative Answer = True\n",
      "2023-08-12 19:09:47.726 | INFO     | __main__:get_response:151 - Modified query: How many paid annual leave I have?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06002499e48d4132917e73fc1bc34376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-12 19:09:47.801 | DEBUG    | __main__:get_response:171 - (Document(page_content='Annual Leave: Eligibility & Entitlement ## You are eligible for annual leave after 3 months of service with the Company. Your entitlement is as stated in your employment contract. You may also refer to the Eligibility & Entitlement tab at this <a href=\"awb://sia.sharepoint.com/sites/Intranet/SitePages/HR-Journey3.aspx?Journey=J02&Topic1=J02-T02&Topic2=J02-T02-04&Topic3=J02-T02-04-01\">page</a> for the annual leave entitlement for the respective staff groups. ', metadata={'REFER': 'EMPTY', 'IMAGE': 'EMPTY', 'URL': nan, 'source': 'S504'}), 0.55194926)\n",
      "2023-08-12 19:09:47.802 | DEBUG    | __main__:get_response:171 - (Document(page_content='Annual Leave: Utilisation ## \\u200bUtilise on one-day or half-day basis within the calendar year in which it is earned; unutilised leave can be carried forward into the next calendar year. As a guide, such carrying forward of leave should not go beyond 31 March of the following year. Any leave unutilised by the end of the following year (2 yearsâ€™ validity) will automatically lapse. Taking of advance leave is strongly discouraged. Where required, you should apply for no-pay leave.', metadata={'REFER': 'EMPTY', 'IMAGE': 'EMPTY', 'URL': nan, 'source': 'S505'}), 0.7212875)\n",
      "2023-08-12 19:09:47.802 | DEBUG    | __main__:get_response:171 - (Document(page_content='How is earned leave calculated for new joiners / staff on contract / staff who have put in their resignation? ## Your prorated annual leave entitlement is generally computed based on the following formula:\\n(Number of calendar days worked / 365 or 366 calendar days) x leave entitlement for the whole calendar year.', metadata={'REFER': 'EMPTY', 'IMAGE': 'EMPTY', 'URL': nan, 'source': 'S14'}), 0.80720145)\n",
      "2023-08-12 19:09:47.802 | DEBUG    | __main__:get_response:171 - (Document(page_content='Annual Leave: Shift worker ## Your annual leave is granted according to the leave roster approved by the respective divisions. Annual leave taken on a day you are rostered for work will be counted as one day of annual leave.\\u200b', metadata={'REFER': 'EMPTY', 'IMAGE': 'EMPTY', 'URL': nan, 'source': 'S507'}), 0.82568747)\n",
      "2023-08-12 19:09:47.803 | DEBUG    | __main__:get_response:171 - (Document(page_content='Annual Leave: Application ## Application, changes and cancellations can be made via myHR / 1SQ > Leave (select time type as Annual Leave). Approval is required before commencement of absences.', metadata={'REFER': 'EMPTY', 'IMAGE': 'EMPTY', 'URL': nan, 'source': 'S510'}), 0.8297821)\n",
      "2023-08-12 19:09:47.804 | INFO     | __main__:get_response:195 - Min distance from Semantic results: 0.5519492626190186\n",
      "2023-08-12 19:09:53.964 | DEBUG    | __main__:get_response:219 - {'output_text': 'The number of paid annual leave you have is stated in your employment contract and may vary depending on your eligibility and entitlement. You can refer to the Eligibility & Entitlement tab on the HR Journey page for the specific annual leave entitlement for your staff group. Annual leave can be utilised on a one-day or half-day basis within the calendar year it is earned, and any unutilised leave can be carried forward into the next calendar year, with a limit of not going beyond March 31st. However, any leave unutilised by the end of the following year will automatically lapse. The calculation of earned leave for new joiners, staff on contract, or those who have put in their resignation is typically based on a prorated formula. As a shift worker, your annual leave is granted according to the approved leave roster, and any annual leave taken on a day you are rostered for work will be counted as one day of annual leave. You can apply for annual leave, make changes, and cancellations through myHR or 1SQ, and approval is required before the commencement of absences.\\nSOURCES: S504, S505, S14, S507, S510'}\n",
      "2023-08-12 19:09:53.968 | INFO     | __main__:get_response:236 - source_part: SOURCES: S504, S505, S14, S507, S510\n",
      "2023-08-12 19:09:53.970 | DEBUG    | __main__:get_response:239 - SOURCES: ['S507', 'S505', 'S504', 'S14', 'S510']\n",
      "2023-08-12 19:09:53.972 | INFO     | __main__:get_response:272 - [FINAL ANSWER]\n",
      "**Disclaimer:**\n",
      "Please note that we are still in a testing phase for Joey-GPT. You may want to validate the answer provided against the cited sources.\n",
      "\n",
      "The number of paid annual leave you have is stated in your employment contract and may vary depending on your eligibility and entitlement. You can refer to the Eligibility & Entitlement tab on the HR Journey page for the specific annual leave entitlement for your staff group. Annual leave can be utilised on a one-day or half-day basis within the calendar year it is earned, and any unutilised leave can be carried forward into the next calendar year, with a limit of not going beyond March 31st. However, any leave unutilised by the end of the following year will automatically lapse. The calculation of earned leave for new joiners, staff on contract, or those who have put in their resignation is typically based on a prorated formula. As a shift worker, your annual leave is granted according to the approved leave roster, and any annual leave taken on a day you are rostered for work will be counted as one day of annual leave. You can apply for annual leave, make changes, and cancellations through myHR or 1SQ, and approval is required before the commencement of absences.\n",
      "\n",
      "5 SOURCES:\n",
      "\n",
      "[1] Annual Leave: Shift worker\n",
      "Your annual leave is granted according to the leave roster approved by the respective divisions. Annual leave taken on a day you are rostered for work will be counted as one day of annual leave.â€‹\n",
      "\n",
      "[2] Annual Leave: Utilisation\n",
      "â€‹Utilise on one-day or half-day basis within the calendar year in which it is earned; unutilised leave can be carried forward into the next calendar year. As a guide, such carrying forward of leave should not go beyond 31 March of the following year. Any leave unutilised by the end of the following year (2 yearsâ€™ validity) will automatically lapse. Taking of advance leave is strongly discouraged. Where required, you should apply for no-pay leave.\n",
      "\n",
      "[3] Annual Leave: Eligibility & Entitlement\n",
      "You are eligible for annual leave after 3 months of service with the Company. Your entitlement is as stated in your employment contract. You may also refer to the Eligibility & Entitlement tab at this <a href=\"awb://sia.sharepoint.com/sites/Intranet/SitePages/HR-Journey3.aspx?Journey=J02&Topic1=J02-T02&Topic2=J02-T02-04&Topic3=J02-T02-04-01\">page</a> for the annual leave entitlement for the respective staff groups.\n",
      "\n",
      "[4] How is earned leave calculated for new joiners / staff on contract / staff who have put in their resignation?\n",
      "Your prorated annual leave entitlement is generally computed based on the following formula:\n",
      "(Number of calendar days worked / 365 or 366 calendar days) x leave entitlement for the whole calendar year.\n",
      "\n",
      "[5] Annual Leave: Application\n",
      "Application, changes and cancellations can be made via myHR / 1SQ > Leave (select time type as Annual Leave). Approval is required before commencement of absences.\n",
      "\n",
      "2023-08-12 19:09:53.975 | DEBUG    | __main__:transformation:431 - {\n",
      "    \"Q0\": {\n",
      "        \"QUESTION\": \"How many paid annual leave I have?\",\n",
      "        \"ANSWER\": \"**Disclaimer:**\\nPlease note that we are still in a testing phase for Joey-GPT. You may want to validate the answer provided against the cited sources.\\n\\nThe number of paid annual leave you have is stated in your employment contract and may vary depending on your eligibility and entitlement. You can refer to the Eligibility & Entitlement tab on the HR Journey page for the specific annual leave entitlement for your staff group. Annual leave can be utilised on a one-day or half-day basis within the calendar year it is earned, and any unutilised leave can be carried forward into the next calendar year, with a limit of not going beyond March 31st. However, any leave unutilised by the end of the following year will automatically lapse. The calculation of earned leave for new joiners, staff on contract, or those who have put in their resignation is typically based on a prorated formula. As a shift worker, your annual leave is granted according to the approved leave roster, and any annual leave taken on a day you are rostered for work will be counted as one day of annual leave. You can apply for annual leave, make changes, and cancellations through myHR or 1SQ, and approval is required before the commencement of absences.\\n\\n5 SOURCES:\\n\\n[1] Annual Leave: Shift worker\\nYour annual leave is granted according to the leave roster approved by the respective divisions. Annual leave taken on a day you are rostered for work will be counted as one day of annual leave.\\u200b\\n\\n[2] Annual Leave: Utilisation\\n\\u200bUtilise on one-day or half-day basis within the calendar year in which it is earned; unutilised leave can be carried forward into the next calendar year. As a guide, such carrying forward of leave should not go beyond 31 March of the following year. Any leave unutilised by the end of the following year (2 years\\u2019 validity) will automatically lapse. Taking of advance leave is strongly discouraged. Where required, you should apply for no-pay leave.\\n\\n[3] Annual Leave: Eligibility & Entitlement\\nYou are eligible for annual leave after 3 months of service with the Company. Your entitlement is as stated in your employment contract. You may also refer to the Eligibility & Entitlement tab at this <a href=\\\"awb://sia.sharepoint.com/sites/Intranet/SitePages/HR-Journey3.aspx?Journey=J02&Topic1=J02-T02&Topic2=J02-T02-04&Topic3=J02-T02-04-01\\\">page</a> for the annual leave entitlement for the respective staff groups.\\n\\n[4] How is earned leave calculated for new joiners / staff on contract / staff who have put in their resignation?\\nYour prorated annual leave entitlement is generally computed based on the following formula:\\n(Number of calendar days worked / 365 or 366 calendar days) x leave entitlement for the whole calendar year.\\n\\n[5] Annual Leave: Application\\nApplication, changes and cancellations can be made via myHR / 1SQ > Leave (select time type as Annual Leave). Approval is required before commencement of absences.\\n\",\n",
      "        \"REFER\": \"EMPTY\",\n",
      "        \"IMAGE\": \"EMPTY\",\n",
      "        \"URL\": \"\",\n",
      "        \"SimScore\": 1\n",
      "    }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Disclaimer:**\n",
      "Please note that we are still in a testing phase for Joey-GPT. You may want to validate the answer provided against the cited sources.\n",
      "\n",
      "The number of paid annual leave you have is stated in your employment contract and may vary depending on your eligibility and entitlement. You can refer to the Eligibility & Entitlement tab on the HR Journey page for the specific annual leave entitlement for your staff group. Annual leave can be utilised on a one-day or half-day basis within the calendar year it is earned, and any unutilised leave can be carried forward into the next calendar year, with a limit of not going beyond March 31st. However, any leave unutilised by the end of the following year will automatically lapse. The calculation of earned leave for new joiners, staff on contract, or those who have put in their resignation is typically based on a prorated formula. As a shift worker, your annual leave is granted according to the approved leave roster, and any annual leave taken on a day you are rostered for work will be counted as one day of annual leave. You can apply for annual leave, make changes, and cancellations through myHR or 1SQ, and approval is required before the commencement of absences.\n",
      "\n",
      "5 SOURCES:\n",
      "\n",
      "[1] Annual Leave: Shift worker\n",
      "Your annual leave is granted according to the leave roster approved by the respective divisions. Annual leave taken on a day you are rostered for work will be counted as one day of annual leave.â€‹\n",
      "\n",
      "[2] Annual Leave: Utilisation\n",
      "â€‹Utilise on one-day or half-day basis within the calendar year in which it is earned; unutilised leave can be carried forward into the next calendar year. As a guide, such carrying forward of leave should not go beyond 31 March of the following year. Any leave unutilised by the end of the following year (2 yearsâ€™ validity) will automatically lapse. Taking of advance leave is strongly discouraged. Where required, you should apply for no-pay leave.\n",
      "\n",
      "[3] Annual Leave: Eligibility & Entitlement\n",
      "You are eligible for annual leave after 3 months of service with the Company. Your entitlement is as stated in your employment contract. You may also refer to the Eligibility & Entitlement tab at this <a href=\"awb://sia.sharepoint.com/sites/Intranet/SitePages/HR-Journey3.aspx?Journey=J02&Topic1=J02-T02&Topic2=J02-T02-04&Topic3=J02-T02-04-01\">page</a> for the annual leave entitlement for the respective staff groups.\n",
      "\n",
      "[4] How is earned leave calculated for new joiners / staff on contract / staff who have put in their resignation?\n",
      "Your prorated annual leave entitlement is generally computed based on the following formula:\n",
      "(Number of calendar days worked / 365 or 366 calendar days) x leave entitlement for the whole calendar year.\n",
      "\n",
      "[5] Annual Leave: Application\n",
      "Application, changes and cancellations can be made via myHR / 1SQ > Leave (select time type as Annual Leave). Approval is required before commencement of absences.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "from urllib.error import URLError\n",
    "import openai\n",
    "import os\n",
    "from collections import deque  \n",
    "from loguru import logger\n",
    "import sys\n",
    "import time\n",
    "import openai\n",
    "# sys.path.append(\"..\")\n",
    "#from config import API_KEY\n",
    "import re\n",
    "import sys\n",
    "import traceback\n",
    "import json\n",
    "from time import time\n",
    "from typing import Callable\n",
    "#import flask\n",
    "# from flask import jsonify\n",
    "from loguru import logger\n",
    "from functools import wraps\n",
    "import pandas as pd\n",
    "\n",
    "# Langchain related\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from hr_ex import HrResult\n",
    "from common import (\n",
    "    GPT_ANSWER_THRESHOLD,\n",
    "    MIN_GPT_QUERY_LENGTH,\n",
    "    MIN_QUERY_LENGTH,\n",
    "    RANKER_NUM_RESULTS,\n",
    "    GPT_MODEL_PARAMS,\n",
    "    MODEL_FOLDER,\n",
    "    GPT_ANSWER_PREFIX,\n",
    ")\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = 'e1ad721cd0fc4e5a89a8c67d1ce6e75d'\n",
    "# os.environ['GPT_API_KEY'] = 'e1ad721cd0fc4e5a89a8c67d1ce6e75d'\n",
    "\n",
    "logger.remove()\n",
    "logger.add(sys.stderr, level=\"DEBUG\")\n",
    "\n",
    "################################# Langchain#####################################\n",
    "# If the GPT api key is not set, then fallback to just the semantic search\n",
    "# if os.getenv(\"GPT_API_KEY\") is None:\n",
    "#     logger.info(\"No GPT3 key, disabling this feature\")\n",
    "#     GPT_STATUS = False\n",
    "# else:\n",
    "#     logger.info(\"GPT3 API Key found\")\n",
    "#     os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"GPT_API_KEY\")\n",
    "#     GPT_STATUS = True\n",
    "GPT_STATUS = True\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set up OpenAI credentials\n",
    "#openai.api_base = \"https://siaec-data-gpt.openai.azure.com/\"\n",
    "#openai.api_type = 'azure'\n",
    "#openai.api_version = \"2023-03-15-preview\"\n",
    "#deployment_name = \"gpt-35-turbo\"\n",
    "#openai.api_key = API_KEY\n",
    "\n",
    "# st.set_page_config(page_title=\"Joey, the HR assistant! ðŸŽ­\", page_icon=\"ðŸŽ­\")\n",
    "# html_temp = \"\"\"\n",
    "# <div style=\"background-color:brown;padding:10px\">\n",
    "# <h2 style=\"color:white;text-align:center;\">Joey, the HR assistant! </h2>\n",
    "# </div>\n",
    "# \"\"\"\n",
    "# st.markdown(html_temp,unsafe_allow_html=True)\n",
    "# st.sidebar.header(\"Joey, the HR assistant!\")\n",
    "\n",
    "\n",
    "# if \"authenticated\" not in st.session_state:\n",
    "#     st.session_state.authenticated = True#False\n",
    "# if \"w2k_hash\" not in st.session_state:\n",
    "#     st.session_state.w2k_hash = \"\"\n",
    "# if \"w2k\" not in st.session_state:\n",
    "#     st.session_state.w2k = \"\"\n",
    "\n",
    "# if \"history\" not in st.session_state:\n",
    "#     st.session_state.history = []\n",
    "#     st.session_state.history.append(\n",
    "#         {\"role\": \"system\", \"content\": \"I'm HR AI assistant. How can i help you today?\"}\n",
    "#     )\n",
    "\n",
    "openai.api_base = \"https://siaec-data-gpt.openai.azure.com/\"\n",
    "openai.api_type = 'azure'\n",
    "openai.api_version = \"2023-03-15-preview\"\n",
    "deployment_name = \"gpt-35-turbo\"\n",
    "# openai.api_key = API_KEY\n",
    "\n",
    "class Chatbot:\n",
    "    def __init__(self) -> None:\n",
    "        self.embeddings = HuggingFaceEmbeddings()\n",
    "        logger.info(f\"Embedding model = {self.embeddings.model_name}\")\n",
    "\n",
    "        logger.info(\"Reading in vectorstore DB from training script\")\n",
    "        self.db = FAISS.load_local(MODEL_FOLDER / \"faiss_index\", self.embeddings)\n",
    "\n",
    "        # 13 Jun 23: Change model to GPT3.5 on our Azure Openai resource\n",
    "        # response = openai.ChatCompletion.create(engine=deployment_name,messages=query,temperature=0)\n",
    "        #         logger.info(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "        # result=\"\"\n",
    "        # result = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "        model = AzureChatOpenAI(\n",
    "            openai_api_base=GPT_MODEL_PARAMS[\"api_base\"],\n",
    "            openai_api_version=GPT_MODEL_PARAMS[\"api_version\"],\n",
    "            deployment_name=GPT_MODEL_PARAMS[\"deployment_name\"],\n",
    "            openai_api_key=\"e1ad721cd0fc4e5a89a8c67d1ce6e75d\",#os.getenv(\"GPT_API_KEY\"),\n",
    "            openai_api_type=GPT_MODEL_PARAMS[\"api_type\"],\n",
    "        )\n",
    "        self.chain = load_qa_with_sources_chain(\n",
    "            model,\n",
    "            chain_type=\"stuff\",\n",
    "        )\n",
    "\n",
    "    def get_response(\n",
    "        self,\n",
    "        query: str,\n",
    "        # hr_grade: str,\n",
    "        database_name: str = \"joey\",\n",
    "        query_gpt: bool = GPT_STATUS,\n",
    "    ) -> dict:\n",
    "        \"\"\"Executes the query and returns the results in the expected format to return\n",
    "        as a response to the user.\n",
    "\n",
    "        :param query: User query\n",
    "        :type query: str\n",
    "        :param hr_grade: User's hr grade, to be passed in the request\n",
    "        :type hr_grade: str\n",
    "        :param database_name: Currently not used; meant to enable querying from multiple vecdbs,\n",
    "        defaults to \"joey\"\n",
    "        :type database_name: str, optional\n",
    "        :param query_gpt: Flag to denote if GPT should be called, defaults to GPT_STATUS\n",
    "        :type query_gpt: bool, optional\n",
    "        :return: A dict of dicts containing the results. Top level key is the answer index,\n",
    "        inner keys are ['Question','Answer','Image','SimScore']\n",
    "        :rtype: dict\n",
    "        \"\"\"\n",
    "\n",
    "        # 28Mar23: Prepare the query\n",
    "        orig_query = query  # make a copy to display in the final output\n",
    "        query = pre_process(query)\n",
    "        logger.info(f\"Modified query: {query}\")\n",
    "\n",
    "        # Semantic similarity results. Outputs are [(doc,score)]\n",
    "        #! distance is returned, not similarity scores\n",
    "        raw_results = self.db.similarity_search_with_score(query, k=RANKER_NUM_RESULTS)\n",
    "\n",
    "        #! Process the results\n",
    "        #! To create the final answer, the best score from the semantic search is\n",
    "        #! extracted; if it's below a threshold, then GPT is called to generate the answer\n",
    "        #! otherwise, the semantic answers are returned. To prepare for this possibility, the\n",
    "        #! semantic answers are also formatted.\n",
    "        docs = []  # To pass to the chain as the context\n",
    "        results = {}  # Formatted results as a dict for potential display\n",
    "        ans_idx = 0  # Initial index for the formatted results\n",
    "        min_distance = 999  # To store the min distance found among the results\n",
    "        all_semantic_questions = (\n",
    "            \"\"  # To keep the questions only. Concatenated as a single string\n",
    "        )\n",
    "        top_answer = \"\"  # To keep the top answer only\n",
    "        for i, raw_result in enumerate(raw_results):\n",
    "            logger.debug(raw_result)\n",
    "\n",
    "            # Get the question,answer and score from the db entries\n",
    "            doc, score = raw_result\n",
    "            (q, a) = doc.page_content.split(\"##\")\n",
    "\n",
    "            if i == 0:\n",
    "                top_answer = q.strip() + \"\\n\" + a.strip() + \"\\n\"\n",
    "            else:\n",
    "                # Keep only the questions\n",
    "                all_semantic_questions += \"- \" + q.strip() + \"\\n\"\n",
    "\n",
    "            docs.append(doc)\n",
    "            results[\"Q\" + str(ans_idx)] = {\n",
    "                \"QUESTION\": q.strip(),\n",
    "                \"ANSWER\": a.strip(),\n",
    "                \"REFER\": doc.metadata[\"REFER\"],\n",
    "                \"IMAGE\": doc.metadata[\"IMAGE\"],\n",
    "                \"URL\": doc.metadata[\"URL\"],\n",
    "                \"SimScore\": score,\n",
    "            }\n",
    "            min_distance = min([min_distance, score])\n",
    "\n",
    "            ans_idx += 1\n",
    "        logger.info(f\"Min distance from Semantic results: {min_distance}\")\n",
    "\n",
    "\n",
    "        #! next blocks are only executed if gpt should be called which are based\n",
    "        #! on the min_distance from the semantic search and the query_gpt flag\n",
    "        #! The output can be 2 types: a) gpt is able to answer the query, b) gpt\n",
    "        #! does not know how to answer the query. In the former case, the gpt\n",
    "        #! response is formatted to extract the answer and the sources (where the\n",
    "        #! text comes from the vecdb). In the latter case, a default answer is given\n",
    "        #! whereby the top answer from the semantic search is returned along with\n",
    "        #! a list of other possible questions to try (comes from the questions in\n",
    "        #! the semantic search)\n",
    "        gpt_answer = GPT_ANSWER_PREFIX\n",
    "        # Call GPT only if the best answer from the semantic search is below threshold:\n",
    "        # i.e. there is a potential answer to the semantic answers\n",
    "        if min_distance <= GPT_ANSWER_THRESHOLD and query_gpt:\n",
    "            # Run the chain for the query\n",
    "            gpt_result = self.chain(\n",
    "                {\n",
    "                    \"input_documents\": docs,\n",
    "                    \"question\": query,\n",
    "                },\n",
    "                return_only_outputs=True,\n",
    "            )\n",
    "            logger.debug(gpt_result)\n",
    "\n",
    "            gpt_answer_text = gpt_result[\"output_text\"]\n",
    "\n",
    "            # gpt_answer_flag is used to indicate if the answer is a valid answer\n",
    "            # if false, a default answer is returned to indicate that the answer\n",
    "            # cannot be found.\n",
    "            if re.search(r\"I don't know\", gpt_answer_text):\n",
    "                gpt_answer_flag = False\n",
    "            else:\n",
    "                if re.search(\"(?=Source|Sources|SOURCE|SOURCES)\", gpt_answer_text):\n",
    "                    gpt_answer_flag = True\n",
    "                    parts = re.split(\n",
    "                        \"(?=Source|Sources|SOURCE|SOURCES)\", gpt_answer_text\n",
    "                    )\n",
    "\n",
    "                    gpt_answer += parts[0].strip() + \"\\n\"\n",
    "                    logger.info(f\"source_part: {parts[1]}\")\n",
    "                    sources = [x for x in re.findall(r\"S\\d+\", parts[1].strip())]\n",
    "                    sources = list(set(sources))\n",
    "                    logger.debug(f\"SOURCES: {sources}\")\n",
    "\n",
    "                    gpt_answer += f\"\\n{len(sources)} SOURCES:\\n\"\n",
    "\n",
    "                    for idx, source in enumerate(sources):\n",
    "                        for doc in docs:\n",
    "                            if source == doc.metadata[\"source\"]:\n",
    "                                (q, a) = doc.page_content.split(\"##\")\n",
    "                                gpt_answer += \"\\n\"\n",
    "                                gpt_answer += f\"[{idx+1}] \" + q.strip() + \"\\n\"\n",
    "                                gpt_answer += a.strip() + \"\\n\"\n",
    "                                gpt_answer = re.sub(r\"\\(S\\d+\\)\", \"\", gpt_answer)\n",
    "                else:\n",
    "                    gpt_answer_flag = False\n",
    "        else:\n",
    "            gpt_answer_flag = False\n",
    "\n",
    "        # If an answer is not found, modify the output by returning the top answer\n",
    "        # (in case it happens to be correct) and a list of possible questions coming\n",
    "        # from the semantic search\n",
    "        if not gpt_answer_flag and query_gpt:\n",
    "            gpt_answer += (\n",
    "                \"Unfortunately, I do not know how to answer your question directly.\\n\"\n",
    "            )\n",
    "            gpt_answer += \"This is the closest answer from the KnowledgeBase:\\n\\n\"\n",
    "            gpt_answer += top_answer\n",
    "            gpt_answer += (\n",
    "                \"\\nAlternatively, you may want to try one of these questions instead:\\n\"\n",
    "            )\n",
    "            gpt_answer += all_semantic_questions\n",
    "            gpt_answer += \"\\nOtherwise, please log a ticket with Joey.\"\n",
    "\n",
    "        if query_gpt:\n",
    "            logger.info(f\"[FINAL ANSWER]\\n{gpt_answer}\")\n",
    "\n",
    "            results = {}\n",
    "            results[\"Q0\"] = {\n",
    "                \"QUESTION\": orig_query.strip(),\n",
    "                \"ANSWER\": gpt_answer,\n",
    "                \"REFER\": \"EMPTY\",\n",
    "                \"IMAGE\": \"EMPTY\",\n",
    "                \"URL\": \"\",\n",
    "                \"SimScore\": 1,\n",
    "            }\n",
    "\n",
    "        return results#hr_extra_result.hr_response(\n",
    "            # pd.DataFrame.from_dict(results, orient=\"index\")\n",
    "        # )\n",
    "\n",
    "\n",
    "############################ HELPER FUNCTIONS ##################################\n",
    "# def send_error_response(error_message: str, container_code: str, error_code: str):\n",
    "#     return error_message,container_code,error_code\n",
    "\n",
    "\n",
    "def measure(func: Callable) -> Callable:\n",
    "    \"\"\"This is a decorator function that measures the execution time of the function\n",
    "    it decorates.\n",
    "\n",
    "    :param func: A function to measure\n",
    "    :type func: Callable\n",
    "    :return: A wrapped function\n",
    "    :rtype: Callable\n",
    "    \"\"\"\n",
    "\n",
    "#    @wraps(func)\n",
    "# def _time_it(*args, **kwargs):\n",
    "#     start = int(round(time() * 1000000))\n",
    "#     try:\n",
    "#         return func(*args, **kwargs)\n",
    "#     finally:\n",
    "#         end_ = int(round(time() * 1000000)) - start\n",
    "#         logger.info(f\"Total execution/response time: {end_ if end_ > 0 else 0} us \")\n",
    "\n",
    "#     return _time_it\n",
    "\n",
    "\n",
    "def count_query_length(text: str) -> int:\n",
    "    \"\"\"Count the number of tokens in the query\n",
    "\n",
    "    :param text: Query\n",
    "    :type text: str\n",
    "    :return: Number of tokens\n",
    "    :rtype: int\n",
    "    \"\"\"\n",
    "\n",
    "    parts = \" \".join(re.compile(r\"\\W+\", re.UNICODE).split(text))\n",
    "    zz = re.sub(\"[^a-zA-Z]+\", \" \", parts).strip()\n",
    "    text_length = len(zz.split(\" \"))\n",
    "    return text_length\n",
    "\n",
    "\n",
    "def pre_process(query: str) -> str:\n",
    "    \"\"\"Modify the query before running the search\n",
    "\n",
    "    :param query: The user query\n",
    "    :type query: str\n",
    "    :return: Modified query\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    if re.search(r\"(travel\\s)?subload\", query):\n",
    "        query = re.sub(\n",
    "            r\"(travel\\s)?subload\", \"leisure travel subject-to-load-basis\", query\n",
    "        )\n",
    "\n",
    "    return query\n",
    "\n",
    "\n",
    "################################# SETUP ########################################\n",
    "logger.info(\"Start of predictor\")\n",
    "logger.info(\"Creating the chatbot database using previously saved document store\")\n",
    "\n",
    "logger.info(\"Creating the chatbot object\")\n",
    "################################# FLASK ########################################\n",
    "# The flask app for serving predictions\n",
    "#app = flask.Flask(__name__)\n",
    "\n",
    "\n",
    "#@app.route(\"/ping\", methods=[\"GET\"])\n",
    "def ping():\n",
    "    status = 200\n",
    "    return \"Ping Successful.\" #flask.Response(\n",
    "#        response=\"Ping Successful.\", status=status, mimetype=\"application/json\"\n",
    "#    )\n",
    "\n",
    "\n",
    "#@app.route(\"/invocations\", methods=[\"POST\"])\n",
    "#@measure\n",
    "def transformation():\n",
    "    try:\n",
    "        logger.info(\"*\" * 20)\n",
    "        logger.info(\"Start /invocations\")\n",
    "\n",
    "        # if flask.request.is_json:\n",
    "        #     data = flask.request.get_json()\n",
    "        #     logger.debug(f\"Json Query packet: {data}\")\n",
    "\n",
    "            # Check required keys in request\n",
    "        # if \"Query\" not in data.keys():\n",
    "        #     return send_error_response(\n",
    "        #         \"Please specify 'Query' in the request.\", 4000, 400\n",
    "        #     )\n",
    "\n",
    "        # if \"Source\" not in data.keys():\n",
    "        #     return send_error_response(\n",
    "        #         \"Please specify 'Source' in the request.\", 4000, 400\n",
    "        #     )\n",
    "\n",
    "        # Return error response if query is too short\n",
    "        query_length = count_query_length(query)\n",
    "        logger.info(f\"Query: {query}\\nQuery length: {query_length}\")\n",
    "        if query_length < MIN_QUERY_LENGTH:\n",
    "            logger.info(\"User query too short. Returning error\")\n",
    "\n",
    "            return send_error_response(\n",
    "                \"Query is too short, please input at least 2 words for the query.\",\n",
    "                4016,\n",
    "                416,\n",
    "            )\n",
    "\n",
    "        # The openai_query flag is to indicate to use GPT answer where possible\n",
    "        # use_openai = False\n",
    "        # use_openai = bool(data[\"openai_query\"])\n",
    "        # if \"openai_query\" in data and str(data[\"openai_query\"]).lower() in [\n",
    "        #     \"true\",\n",
    "        #     \"1\",\n",
    "        #     \"y\",\n",
    "        #     \"yes\",\n",
    "        # ]:\n",
    "        use_openai = True\n",
    "        logger.info(f\"Use Generative Answer = {use_openai}\")\n",
    "\n",
    "        if query_length < MIN_GPT_QUERY_LENGTH:\n",
    "            \"\"\"\n",
    "            If the query is too short, the Info Retrieval search could match\n",
    "            easily to many docs in the KB because the intent of the query\n",
    "            is not clear. This in turn will create a Context with potentially\n",
    "            irrelevant info. When sent to GPT, a low quality answer could\n",
    "            result.\n",
    "            The parameter MIN_GPT_QUERY_LENGTH is set in common.py is currently\n",
    "            from heuristics. Can be adjusted higher if shorter queries still\n",
    "            gives poor GPT results.\n",
    "            \"\"\"\n",
    "            logger.info(\"Query is too short for GPT\")\n",
    "            use_openai = False  # overrides the request flag\n",
    "\n",
    "        result = kris_chat.get_response(\n",
    "            query,\n",
    "            database_name=\"Joey\",\n",
    "            query_gpt=use_openai,\n",
    "        )\n",
    "\n",
    "        logger.debug(json.dumps(result, indent=4))\n",
    "\n",
    "        return print(result[\"Q0\"][\"ANSWER\"])#flask.Response(\n",
    "#                response=json.dumps(result), status=200, mimetype=\"application/json\"\n",
    "#            )\n",
    "\n",
    "        # else:\n",
    "        #     return send_error_response(\"Only JSON format is supported.\", 4000, 400)\n",
    "    except Exception:\n",
    "        traceback.print_exc()\n",
    "        logger.exception(\"Error is transformation() function\")\n",
    "        return print(\"Sorry not sure if I fully understand- please elaborate your question.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# if st.session_state.authenticated is True:\n",
    "#     # auth=st.session_state.authenticated\n",
    "#     st.write(\"\"\"When asking a question, be as detailed as possible. Ex: if you want to ask about medical leaves, ask: Am I eligible to apply for under medical leave? If your question is not very specific, Joey will share the closest questions with answers that match your query.\"\"\")\n",
    "\n",
    "    # st.write(\"\"\"Please input your prompt here, if your prompt is quite long, can use the handle on bottom right to adjust the input box height.\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "    # with st.form(\"gpt_form\"):\n",
    "    #     query = st.text_area(\"Please input your query here.\",\n",
    "    #         \"\")\n",
    "    #     submit_button = st.form_submit_button(\"Send\", type='primary')\n",
    "    #     if submit_button and query:\n",
    "    #         start_time = time.time()\n",
    "    #         st.session_state.history.append({\"role\": \"user\", \"content\": query})\n",
    "query='How many paid annual leave I have?'\n",
    "kris_chat = Chatbot()\n",
    "res=transformation()\n",
    "\n",
    "# used to modify the output based on user grade. Currently this is a passthrough\n",
    "# hr_extra_result = HrResult()\n",
    "\n",
    "#             try:\n",
    "#                 response = openai.ChatCompletion.create(engine=deployment_name,messages=st.session_state.history,temperature=0)\n",
    "#                 logger.info(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "#                 result=\"\"\n",
    "#                 result = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "#                 st.success('{}'.format(result))\n",
    "#                 end_time = time.time()\n",
    "#                 elapsed_time = end_time - start_time\n",
    "\n",
    "#                 # Add assistant response to the conversation history and limit to 5 messages\n",
    "#                 st.session_state.history.append(\n",
    "#                     {\"role\": \"assistant\", \"content\": result}\n",
    "#                 )\n",
    "#                 if len(st.session_state.history) > 10:\n",
    "#                     st.session_state.history.pop(0)\n",
    "#                     st.session_state.history.pop(0)\n",
    "#                 # token count\n",
    "#                 prompt_token_count = int(response[\"usage\"][\"prompt_tokens\"])\n",
    "#                 answer_token_count = int(response[\"usage\"][\"completion_tokens\"])\n",
    "#                 # how many seconds used for the whole query\n",
    "#                 query_time = round(elapsed_time, 2)\n",
    "\n",
    "#                 # logger.info(\n",
    "#                 #     f\"{st.session_state.w2k_hash}|{AZURE_ENGINE}|{user_input}|{assistant_response}|Prompt_token_count:{str(prompt_token_count)}|Answer_token_count:{(answer_token_count)}|Query_time:{(query_time)}\"\n",
    "#                 # )\n",
    "#                 # for saa gamification\n",
    "#                 logger.info(\n",
    "#                     f\"{st.session_state.w2k}|{deployment_name}|{defect_text}|{result}|Prompt_token_count:{str(prompt_token_count)}|Answer_token_count:{(answer_token_count)}|Query_time:{(query_time)}\"\n",
    "#                 )\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 logger.error(e)\n",
    "#                 st.warning(\n",
    "#                     \"We apologize for the inconvenience. The Microsoft service is currently beyond capacity. We have sent a message to our Data team to investigate and we hope to have it resolved soon. Please check back later for updates. Thank you for your patience.\"\n",
    "#                 )\n",
    "#     acco = st.expander(\"Conversation history\", expanded=True)\n",
    "#     for message in st.session_state.history:\n",
    "#         acco.write(f'{message[\"role\"].capitalize()}: {message[\"content\"]}')\n",
    "\n",
    "\n",
    "#     if st.button(\"About\"):\n",
    "# #        st.text(\"Lets Learn\")\n",
    "#         st.text(\"Please reach out to sahil_sharma for feedback.\")\n",
    "# else: st.warning(\"Please login from the main page first\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6cffc3d1-40ce-48f8-a733-e9abbf5db4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c7966d-955f-44ce-a766-bd22e84bbc82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3b6ea7-16dd-4eeb-bb60-ead45a701fed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be381f59-df82-4929-82a2-921a035880b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bebe8c7f-9d19-42c5-8173-7b953be0052a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-07 23:31:31.676 INFO    sentence_transformers.SentenceTransformer: Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "2023-08-07 23:31:32.341 INFO    sentence_transformers.SentenceTransformer: Use pytorch device: cpu\n",
      "2023-08-07 23:31:32.342 | INFO     | __main__:__init__:97 - Embedding model = sentence-transformers/all-mpnet-base-v2\n",
      "2023-08-07 23:31:32.342 | INFO     | __main__:__init__:99 - Reading in vectorstore DB from training script\n",
      "2023-08-07 23:31:32.349 | INFO     | __main__:get_response:138 - Modified query: What if I have used up my annual leave?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bai\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad730636fab44abe83dd263f4ba36a1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-07 23:31:32.415 | DEBUG    | __main__:get_response:159 - (Document(page_content='Annual Leave: Utilisation ## \\u200bUtilise on one-day or half-day basis within the calendar year in which it is earned; unutilised leave can be carried forward into the next calendar year. As a guide, such carrying forward of leave should not go beyond 31 March of the following year. Any leave unutilised by the end of the following year (2 yearsâ€™ validity) will automatically lapse. Taking of advance leave is strongly discouraged. Where required, you should apply for no-pay leave.', metadata={'REFER': 'EMPTY', 'IMAGE': 'EMPTY', 'URL': nan, 'source': 'S505'}), 0.5175082)\n",
      "2023-08-07 23:31:32.415 | DEBUG    | __main__:get_response:159 - (Document(page_content='Annual Leave: Eligibility & Entitlement ## You are eligible for annual leave after 3 months of service with the Company. Your entitlement is as stated in your employment contract. You may also refer to the Eligibility & Entitlement tab at this <a href=\"awb://sia.sharepoint.com/sites/Intranet/SitePages/HR-Journey3.aspx?Journey=J02&Topic1=J02-T02&Topic2=J02-T02-04&Topic3=J02-T02-04-01\">page</a> for the annual leave entitlement for the respective staff groups. ', metadata={'REFER': 'EMPTY', 'IMAGE': 'EMPTY', 'URL': nan, 'source': 'S504'}), 0.6652296)\n",
      "2023-08-07 23:31:32.416 | DEBUG    | __main__:get_response:159 - (Document(page_content='Annual Leave: Application ## Application, changes and cancellations can be made via myHR / 1SQ > Leave (select time type as Annual Leave). Approval is required before commencement of absences.', metadata={'REFER': 'EMPTY', 'IMAGE': 'EMPTY', 'URL': nan, 'source': 'S510'}), 0.70539165)\n",
      "2023-08-07 23:31:32.416 | DEBUG    | __main__:get_response:159 - (Document(page_content='Can I carry forward my annual leave to the following year? ## As a good practice, you should utilise your annual leave by 31 December of the current year. \\nIf you are unable to finish utilising your annual leave within the calendar year due to operational reasons, you may discuss with your department head for your leave to be cleared by 31 March the following year.', metadata={'REFER': 'EMPTY', 'IMAGE': 'EMPTY', 'URL': nan, 'source': 'S9'}), 0.72632277)\n",
      "2023-08-07 23:31:32.416 | DEBUG    | __main__:get_response:159 - (Document(page_content='Annual Leave: Retired & re-employed employee ## You should utilise all earned annual leave upon retirement and before the start of the re-employment contract, unless there are exceptional circumstances to warrant otherwise. Annual leave entitlement offered as per your re-employment contract must be utilised within the contract year.\\u200b', metadata={'REFER': 'EMPTY', 'IMAGE': 'EMPTY', 'URL': nan, 'source': 'S509'}), 0.74078906)\n",
      "2023-08-07 23:31:32.417 | INFO     | __main__:get_response:183 - Min distance from Semantic results: 0.5175082087516785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baibai\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "from urllib.error import URLError\n",
    "import openai\n",
    "import os\n",
    "from collections import deque  \n",
    "from loguru import logger\n",
    "import sys\n",
    "import time\n",
    "# sys.path.append(\"..\")\n",
    "#from config import API_KEY\n",
    "import re\n",
    "import sys\n",
    "import traceback\n",
    "import json\n",
    "from time import time\n",
    "from typing import Callable\n",
    "#import flask\n",
    "# from flask import jsonify\n",
    "from loguru import logger\n",
    "from functools import wraps\n",
    "import pandas as pd\n",
    "\n",
    "# Langchain related\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from hr_ex import HrResult\n",
    "from common import (\n",
    "    GPT_ANSWER_THRESHOLD,\n",
    "    MIN_GPT_QUERY_LENGTH,\n",
    "    MIN_QUERY_LENGTH,\n",
    "    RANKER_NUM_RESULTS,\n",
    "    GPT_MODEL_PARAMS,\n",
    "    MODEL_FOLDER,\n",
    "    GPT_ANSWER_PREFIX,\n",
    ")\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = 'e1ad721cd0fc4e5a89a8c67d1ce6e75d'\n",
    "# os.environ['GPT_API_KEY'] = 'e1ad721cd0fc4e5a89a8c67d1ce6e75d'\n",
    "\n",
    "logger.remove()\n",
    "logger.add(sys.stderr, level=\"DEBUG\")\n",
    "\n",
    "################################# Langchain#####################################\n",
    "# If the GPT api key is not set, then fallback to just the semantic search\n",
    "# if os.getenv(\"GPT_API_KEY\") is None:\n",
    "#     logger.info(\"No GPT3 key, disabling this feature\")\n",
    "#     GPT_STATUS = False\n",
    "# else:\n",
    "#     logger.info(\"GPT3 API Key found\")\n",
    "#     os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"GPT_API_KEY\")\n",
    "#     GPT_STATUS = True\n",
    "GPT_STATUS = True\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set up OpenAI credentials\n",
    "#openai.api_base = \"https://siaec-data-gpt.openai.azure.com/\"\n",
    "#openai.api_type = 'azure'\n",
    "#openai.api_version = \"2023-03-15-preview\"\n",
    "#deployment_name = \"gpt-35-turbo\"\n",
    "#openai.api_key = API_KEY\n",
    "\n",
    "# st.set_page_config(page_title=\"Joey, the HR assistant! ðŸŽ­\", page_icon=\"ðŸŽ­\")\n",
    "# html_temp = \"\"\"\n",
    "# <div style=\"background-color:brown;padding:10px\">\n",
    "# <h2 style=\"color:white;text-align:center;\">Joey, the HR assistant! </h2>\n",
    "# </div>\n",
    "# \"\"\"\n",
    "# st.markdown(html_temp,unsafe_allow_html=True)\n",
    "# st.sidebar.header(\"Joey, the HR assistant!\")\n",
    "\n",
    "\n",
    "# if \"authenticated\" not in st.session_state:\n",
    "#     st.session_state.authenticated = True#False\n",
    "# if \"w2k_hash\" not in st.session_state:\n",
    "#     st.session_state.w2k_hash = \"\"\n",
    "# if \"w2k\" not in st.session_state:\n",
    "#     st.session_state.w2k = \"\"\n",
    "\n",
    "# if \"history\" not in st.session_state:\n",
    "#     st.session_state.history = []\n",
    "#     st.session_state.history.append(\n",
    "#         {\"role\": \"system\", \"content\": \"I'm HR AI assistant. How can i help you today?\"}\n",
    "#     )\n",
    "\n",
    "\n",
    "\n",
    "class Chatbot:\n",
    "    def __init__(self) -> None:\n",
    "        self.embeddings = HuggingFaceEmbeddings()\n",
    "        logger.info(f\"Embedding model = {self.embeddings.model_name}\")\n",
    "\n",
    "        logger.info(\"Reading in vectorstore DB from training script\")\n",
    "        self.db = FAISS.load_local(MODEL_FOLDER / \"faiss_index\", self.embeddings)\n",
    "\n",
    "        # 13 Jun 23: Change model to GPT3.5 on our Azure Openai resource\n",
    "        model = AzureChatOpenAI(\n",
    "            openai_api_base=GPT_MODEL_PARAMS[\"api_base\"],\n",
    "            openai_api_version=GPT_MODEL_PARAMS[\"api_version\"],\n",
    "            deployment_name=GPT_MODEL_PARAMS[\"deployment_name\"],\n",
    "            openai_api_key=\"e1ad721cd0fc4e5a89a8c67d1ce6e75d\",#os.getenv(\"GPT_API_KEY\"),\n",
    "            openai_api_type=GPT_MODEL_PARAMS[\"api_type\"],\n",
    "        )\n",
    "        self.chain = load_qa_with_sources_chain(\n",
    "            model,\n",
    "            chain_type=\"stuff\",\n",
    "        )\n",
    "\n",
    "    def get_response(\n",
    "        self,\n",
    "        query: str\n",
    "    ) -> dict:\n",
    "        \"\"\"Executes the query and returns the results in the expected format to return\n",
    "        as a response to the user.\n",
    "\n",
    "        :param query: User query\n",
    "        :type query: str\n",
    "        :param hr_grade: User's hr grade, to be passed in the request\n",
    "        :type hr_grade: str\n",
    "        :param database_name: Currently not used; meant to enable querying from multiple vecdbs,\n",
    "        defaults to \"joey\"\n",
    "        :type database_name: str, optional\n",
    "        :param query_gpt: Flag to denote if GPT should be called, defaults to GPT_STATUS\n",
    "        :type query_gpt: bool, optional\n",
    "        :return: A dict of dicts containing the results. Top level key is the answer index,\n",
    "        inner keys are ['Question','Answer','Image','SimScore']\n",
    "        :rtype: dict\n",
    "        \"\"\"\n",
    "\n",
    "        # 28Mar23: Prepare the query\n",
    "        orig_query = query  # make a copy to display in the final output\n",
    "        logger.info(f\"Modified query: {query}\")\n",
    "\n",
    "        # Semantic similarity results. Outputs are [(doc,score)]\n",
    "        #! distance is returned, not similarity scores\n",
    "        raw_results = self.db.similarity_search_with_score(query, k=RANKER_NUM_RESULTS)\n",
    "        print(\"baibai\")\n",
    "\n",
    "        #! Process the results\n",
    "        #! To create the final answer, the best score from the semantic search is\n",
    "        #! extracted; if it's below a threshold, then GPT is called to generate the answer\n",
    "        #! otherwise, the semantic answers are returned. To prepare for this possibility, the\n",
    "        #! semantic answers are also formatted.\n",
    "        docs = []  # To pass to the chain as the context\n",
    "        results = {}  # Formatted results as a dict for potential display\n",
    "        ans_idx = 0  # Initial index for the formatted results\n",
    "        min_distance = 999  # To store the min distance found among the results\n",
    "        all_semantic_questions = (\n",
    "            \"\"  # To keep the questions only. Concatenated as a single string\n",
    "        )\n",
    "        top_answer = \"\"  # To keep the top answer only\n",
    "        for i, raw_result in enumerate(raw_results):\n",
    "            logger.debug(raw_result)\n",
    "\n",
    "            # Get the question,answer and score from the db entries\n",
    "            doc, score = raw_result\n",
    "            (q, a) = doc.page_content.split(\"##\")\n",
    "\n",
    "            if i == 0:\n",
    "                top_answer = q.strip() + \"\\n\" + a.strip() + \"\\n\"\n",
    "            else:\n",
    "                # Keep only the questions\n",
    "                all_semantic_questions += \"- \" + q.strip() + \"\\n\"\n",
    "\n",
    "            docs.append(doc)\n",
    "            results[\"Q\" + str(ans_idx)] = {\n",
    "                \"QUESTION\": q.strip(),\n",
    "                \"ANSWER\": a.strip(),\n",
    "                \"REFER\": doc.metadata[\"REFER\"],\n",
    "                \"IMAGE\": doc.metadata[\"IMAGE\"],\n",
    "                \"URL\": doc.metadata[\"URL\"],\n",
    "                \"SimScore\": score,\n",
    "            }\n",
    "            min_distance = min([min_distance, score])\n",
    "\n",
    "            ans_idx += 1\n",
    "        logger.info(f\"Min distance from Semantic results: {min_distance}\")\n",
    "        return results#[\"Q0\"][\"ANSWER\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def transformation():\n",
    "    # try:\n",
    "    print(\"bai\")\n",
    "    result = kris_chat.get_response(query)\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "query='What if I have used up my annual leave?'\n",
    "kris_chat = Chatbot()\n",
    "res=transformation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03dd3956-e06f-4667-a675-3f44764a32ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Q0': {'QUESTION': 'Annual Leave: Utilisation',\n",
       "  'ANSWER': '\\u200bUtilise on one-day or half-day basis within the calendar year in which it is earned; unutilised leave can be carried forward into the next calendar year. As a guide, such carrying forward of leave should not go beyond 31 March of the following year. Any leave unutilised by the end of the following year (2 yearsâ€™ validity) will automatically lapse. Taking of advance leave is strongly discouraged. Where required, you should apply for no-pay leave.',\n",
       "  'REFER': 'EMPTY',\n",
       "  'IMAGE': 'EMPTY',\n",
       "  'URL': nan,\n",
       "  'SimScore': 0.5175082},\n",
       " 'Q1': {'QUESTION': 'Annual Leave: Eligibility & Entitlement',\n",
       "  'ANSWER': 'You are eligible for annual leave after 3 months of service with the Company. Your entitlement is as stated in your employment contract. You may also refer to the Eligibility & Entitlement tab at this <a href=\"awb://sia.sharepoint.com/sites/Intranet/SitePages/HR-Journey3.aspx?Journey=J02&Topic1=J02-T02&Topic2=J02-T02-04&Topic3=J02-T02-04-01\">page</a> for the annual leave entitlement for the respective staff groups.',\n",
       "  'REFER': 'EMPTY',\n",
       "  'IMAGE': 'EMPTY',\n",
       "  'URL': nan,\n",
       "  'SimScore': 0.6652296},\n",
       " 'Q2': {'QUESTION': 'Annual Leave: Application',\n",
       "  'ANSWER': 'Application, changes and cancellations can be made via myHR / 1SQ > Leave (select time type as Annual Leave). Approval is required before commencement of absences.',\n",
       "  'REFER': 'EMPTY',\n",
       "  'IMAGE': 'EMPTY',\n",
       "  'URL': nan,\n",
       "  'SimScore': 0.70539165},\n",
       " 'Q3': {'QUESTION': 'Can I carry forward my annual leave to the following year?',\n",
       "  'ANSWER': 'As a good practice, you should utilise your annual leave by 31 December of the current year. \\nIf you are unable to finish utilising your annual leave within the calendar year due to operational reasons, you may discuss with your department head for your leave to be cleared by 31 March the following year.',\n",
       "  'REFER': 'EMPTY',\n",
       "  'IMAGE': 'EMPTY',\n",
       "  'URL': nan,\n",
       "  'SimScore': 0.72632277},\n",
       " 'Q4': {'QUESTION': 'Annual Leave: Retired & re-employed employee',\n",
       "  'ANSWER': 'You should utilise all earned annual leave upon retirement and before the start of the re-employment contract, unless there are exceptional circumstances to warrant otherwise. Annual leave entitlement offered as per your re-employment contract must be utilised within the contract year.\\u200b',\n",
       "  'REFER': 'EMPTY',\n",
       "  'IMAGE': 'EMPTY',\n",
       "  'URL': nan,\n",
       "  'SimScore': 0.74078906}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca2583de-291a-4dc7-afba-8e078a6998fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-08 22:22:54.391 Session state does not function when running a script without `streamlit run`\n",
      "2023-08-08 22:22:54.393 Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "2023-08-08 22:22:55.037 Use pytorch device: cpu\n",
      "2023-08-08 22:22:55.037 | INFO     | __main__:__init__:98 - Embedding model = sentence-transformers/all-mpnet-base-v2\n",
      "2023-08-08 22:22:55.038 | INFO     | __main__:__init__:99 - Reading in vectorstore DB from training script\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check1\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "from urllib.error import URLError\n",
    "import openai\n",
    "import os\n",
    "from collections import deque\n",
    "from loguru import logger\n",
    "import sys\n",
    "import time\n",
    "# sys.path.append(\"..\")\n",
    "#from config import API_KEY\n",
    "import re\n",
    "import sys\n",
    "import traceback\n",
    "import json\n",
    "from time import time\n",
    "from typing import Callable\n",
    "#import flask\n",
    "# from flask import jsonify\n",
    "from loguru import logger\n",
    "from functools import wraps\n",
    "import pandas as pd\n",
    "\n",
    "# Langchain related\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "#from hr_ex import HrResult\n",
    "from common import (\n",
    "    GPT_ANSWER_THRESHOLD,\n",
    "    MIN_GPT_QUERY_LENGTH,\n",
    "    MIN_QUERY_LENGTH,\n",
    "    RANKER_NUM_RESULTS,\n",
    "    GPT_MODEL_PARAMS,\n",
    "    MODEL_FOLDER,\n",
    "    GPT_ANSWER_PREFIX,\n",
    ")\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = 'e1ad721cd0fc4e5a89a8c67d1ce6e75d'\n",
    "# os.environ['GPT_API_KEY'] = 'e1ad721cd0fc4e5a89a8c67d1ce6e75d'\n",
    "\n",
    "logger.remove()\n",
    "logger.add(sys.stderr, level=\"DEBUG\")\n",
    "\n",
    "################################# Langchain#####################################\n",
    "# If the GPT api key is not set, then fallback to just the semantic search\n",
    "# if os.getenv(\"GPT_API_KEY\") is None:\n",
    "#     logger.info(\"No GPT3 key, disabling this feature\")\n",
    "#     GPT_STATUS = False\n",
    "# else:\n",
    "#     logger.info(\"GPT3 API Key found\")\n",
    "#     os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"GPT_API_KEY\")\n",
    "#     GPT_STATUS = True\n",
    "GPT_STATUS = True\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Set up OpenAI credentials\n",
    "# #openai.api_base = \"https://siaec-data-gpt.openai.azure.com/\"\n",
    "# #openai.api_type = 'azure'\n",
    "# #openai.api_version = \"2023-03-15-preview\"\n",
    "# #deployment_name = \"gpt-35-turbo\"\n",
    "# #openai.api_key = API_KEY\n",
    "\n",
    "st.set_page_config(page_title=\"Joey, the HR assistant! ðŸŽ­\", page_icon=\"ðŸŽ­\")\n",
    "html_temp = \"\"\"\n",
    "<div style=\"background-color:brown;padding:10px\">\n",
    "<h2 style=\"color:white;text-align:center;\">Joey, the HR assistant! </h2>\n",
    "</div>\n",
    "\"\"\"\n",
    "st.markdown(html_temp,unsafe_allow_html=True)\n",
    "st.sidebar.header(\"Joey, the HR assistant!\")\n",
    "\n",
    "\n",
    "st.session_state.authenticated = True\n",
    "if \"authenticated\" not in st.session_state:\n",
    "    st.session_state.authenticated = True#False\n",
    "if \"w2k_hash\" not in st.session_state:\n",
    "    st.session_state.w2k_hash = \"\"\n",
    "if \"w2k\" not in st.session_state:\n",
    "    st.session_state.w2k = \"\"\n",
    "\n",
    "#if \"history\" not in st.session_state:\n",
    "#    st.session_state.history = []\n",
    "#    st.session_state.history.append(\n",
    "#        {\"role\": \"system\", \"content\": \"I'm an HR AI assistant. How can i help you today?\"}\n",
    "#    )\n",
    "\n",
    "\n",
    "\n",
    "class Chatbot:\n",
    "    def __init__(self) -> None:\n",
    "        self.embeddings = HuggingFaceEmbeddings()\n",
    "        logger.info(f\"Embedding model = {self.embeddings.model_name}\")\n",
    "        logger.info(\"Reading in vectorstore DB from training script\")\n",
    "        self.db = FAISS.load_local(\"/Users/sahil_sharma/Desktop/Joey 2.0/streamlit/faiss_index\", self.embeddings)\n",
    "#        self.db = FAISS.load_local(MODEL_FOLDER / \"faiss_index\", self.embeddings)\n",
    "        print(\"check1\")\n",
    "\n",
    "k=Chatbot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3a6df65-b9f8-4e11-9af6-51c6c25b601e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-10 16:21:52.164 | INFO     | __main__:<module>:61 - Starting SmartSearch training\n",
      "2023-08-10 16:21:52.166 | INFO     | __main__:train:27 - Read in qa file as a DataFrame\n",
      "2023-08-10 16:21:52.189 | INFO     | __main__:train:39 - Creating Documents from the DataFrame\n",
      "2023-08-10 16:21:52.192 | INFO     | __main__:train:43 - Loading in DataFrame in Vectorstore\n",
      "2023-08-10 16:21:52.193 Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "2023-08-10 16:21:53.115 Use pytorch device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8afc1e63a2844847a6d9fe58bcd9b88e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-10 16:22:02.646 | INFO     | __main__:train:51 - Success: FAISS saved to /Users/sahil_sharma/Desktop/Joey 2.0/streamlit/data/faiss_index_ec\n",
      "2023-08-10 16:22:02.649 | INFO     | __main__:<module>:65 - Training completed\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import shutil\n",
    "from loguru import logger\n",
    "\n",
    "from common import DATA_FILE, MODEL_FOLDER\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "\n",
    "def train() -> bool:\n",
    "    \"\"\"Train script for KrisML.\n",
    "    The script just ingests the input document which is an Excel spreadsheet\n",
    "    as a vectordb. The question and answer columns are first concatenated as the\n",
    "    'text' columns. The other columns are inserted as metadata for each row.\n",
    "\n",
    "    :return: True\n",
    "    :rtype: bool\n",
    "    \"\"\"\n",
    "    # Copy the input file so that the inference container can access it\n",
    "    # shutil.copy(DATA_FILE, MODEL_FOLDER)\n",
    "    # logger.info(f\"Input file copied to {MODEL_FOLDER}\")\n",
    "\n",
    "    logger.info(\"Read in qa file as a DataFrame\")\n",
    "    input_df = pd.read_excel('/Users/sahil_sharma/Desktop/Joey 2.0/streamlit/data/SIAEC_HR.xlsx')\n",
    "    # Pre-processing:\n",
    "    # 1) combine the question and answer into one text column\n",
    "    # 2) add a source column using the index. This will be used at the metadata\n",
    "    # in the vectorstore\n",
    "    input_df = (\n",
    "        input_df.assign(text=lambda df_: df_[\"QUESTION\"] + \" ## \" + df_[\"ANSWER\"])\n",
    "        .assign(source=lambda df_: \"S\" + df_.index.astype(\"str\"))\n",
    "        .drop([\"QUESTION\", \"ANSWER\"], axis=1)\n",
    "    )\n",
    "\n",
    "    logger.info(\"Creating Documents from the DataFrame\")\n",
    "    loader = DataFrameLoader(input_df, page_content_column=\"text\")\n",
    "    docs = loader.load()\n",
    "\n",
    "    logger.info(\"Loading in DataFrame in Vectorstore\")\n",
    "    embeddings = HuggingFaceEmbeddings()\n",
    "    db = FAISS.from_documents(\n",
    "        docs,\n",
    "        embeddings,\n",
    "    )\n",
    "    # db.save_local(MODEL_FOLDER / \"faiss_index\")\n",
    "    db.save_local(\"/Users/sahil_sharma/Desktop/Joey 2.0/streamlit/data/faiss_index_ec\")\n",
    "    logger.info(f\"Success: FAISS saved to {'/Users/sahil_sharma/Desktop/Joey 2.0/streamlit/data/faiss_index_ec'}\")\n",
    "\n",
    "    # FYI: At the end of the KrisML training process, the files located in the MODEL_FOLDER\n",
    "    # are zipped together and saved in the model_outputs folder in the s3 bucket. These\n",
    "    # files are then used by the inference container to load the model and perform inference.\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"Starting SmartSearch training\")\n",
    "\n",
    "    train()\n",
    "\n",
    "    logger.info(\"Training completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53664ba7-6c8e-4955-937a-f017ff35c147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aacc'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=\"aa\"\n",
    "a+=\"cc\"\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328ba733-a427-4683-bfbe-2408372ea05f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
