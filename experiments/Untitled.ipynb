{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20066f6c-73a7-4d29-b53e-38106cef891b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-08 16:47:31.089 | INFO     | __main__:<module>:339 - Start of predictor\n",
      "2023-08-08 16:47:31.089 | INFO     | __main__:<module>:340 - Creating the chatbot database using previously saved document store\n",
      "2023-08-08 16:47:31.090 | INFO     | __main__:<module>:342 - Creating the chatbot object\n",
      "2023-08-08 16:47:31.091 INFO    sentence_transformers.SentenceTransformer: Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "2023-08-08 16:47:31.955 INFO    sentence_transformers.SentenceTransformer: Use pytorch device: cpu\n",
      "2023-08-08 16:47:31.956 | INFO     | __main__:__init__:97 - Embedding model = sentence-transformers/all-mpnet-base-v2\n",
      "2023-08-08 16:47:31.956 | INFO     | __main__:__init__:99 - Reading in vectorstore DB from training script\n",
      "2023-08-08 16:47:31.960 | INFO     | __main__:transformation:360 - ********************\n",
      "2023-08-08 16:47:31.961 | INFO     | __main__:transformation:361 - Start /invocations\n",
      "2023-08-08 16:47:31.961 | INFO     | __main__:transformation:380 - Query: How many days of medical leave I can take for critical illness?\n",
      "Query length: 12\n",
      "2023-08-08 16:47:31.961 | INFO     | __main__:transformation:400 - Use Generative Answer = True\n",
      "2023-08-08 16:47:31.962 | INFO     | __main__:get_response:142 - Modified query: How many days of medical leave I can take for critical illness?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e3c5332996a485fb65778851e6b55b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-08 16:47:32.075 | DEBUG    | __main__:get_response:162 - (Document(page_content='Compassionate Leave: Conditions ## You can take compassionate leave for the occasion(s) that such leave is required for the event\\nCompassionate occasions or reasons are defined as the critical illness or death of a parent, spouse, child, brother, sister, grandparent or parent-in-law. â€˜Critical illnessâ€™ is defined as illness of a nature warranting the patient to be listed on the â€˜Dangerously ill listâ€™ of a hospital.', metadata={'REFER': 'EMPTY', 'IMAGE': 'EMPTY', 'URL': nan, 'source': 'S544'}), 0.74282956)\n",
      "2023-08-08 16:47:32.075 | DEBUG    | __main__:get_response:162 - (Document(page_content=\"Medical Leave: Additional hospitalisation leave for critical illnesses ## You are eligible for additional paid medical leave if you are suffering from tuberculosis, cancer, AIDS, stroke or heart attack.\\nUp to 3 years' service: 3 months' full pay + 3 months' half pay.\\n3 to 4 years' service: 4 months' full pay\\xa0+ 2 months' half pay.\\n4 to 5 years' service: 5 months' full pay + 2 months' half pay.\\nMore than 5 years' service: 6 months' full pay + 2 months' half pay\\u200b.\", metadata={'REFER': 'EMPTY', 'IMAGE': 'EMPTY', 'URL': nan, 'source': 'S456'}), 0.74540746)\n",
      "2023-08-08 16:47:32.076 | DEBUG    | __main__:get_response:162 - (Document(page_content='Medical Leave: Outpatient MC: Application ## Ground Staff: Inform your departmental head if you fall sick and are granted a medical certificate (MC). If the MC is issued by a panel GP / dental clinic, it will be updated automatically. If the MC is issued by polyclinics, A&Es or non-panel clinics, apply via myHR / 1SQ with submission of medical certificate. \\n\\nCabin Crew & Pilots: Inform Cabin Crew / Flight Ops Control Centre respectively and follow up with the submission of your MC as soon as possible.\\n\\nIf you are absent continuously for more than 48 hours without informing the Company, your absence will be considered as absent without leave.', metadata={'REFER': 'EMPTY', 'IMAGE': 'EMPTY', 'URL': nan, 'source': 'S449'}), 0.87355644)\n",
      "2023-08-08 16:47:32.076 | DEBUG    | __main__:get_response:162 - (Document(page_content='Who should I inform if I am sick / unwell / on medical leave? ## If you are sick and unable to report to work, you will need to inform the following within 48 hours and submit your medical certificate as soon as possible.', metadata={'REFER': 'MC Reporting', 'IMAGE': 'EMPTY', 'URL': nan, 'source': 'S0'}), 0.87667334)\n",
      "2023-08-08 16:47:32.076 | DEBUG    | __main__:get_response:162 - (Document(page_content='Medical Leave: Outpatient MC: Number of paid outpatient leave per calendar year ## Unless otherwise stated in your employment contract, you are eligible for 28 days per calendar year. If you exceed this quota, your outpatient medical leave will be treated as medical no-pay leave.\\u200b\\n', metadata={'REFER': 'EMPTY', 'IMAGE': 'EMPTY', 'URL': nan, 'source': 'S450'}), 0.8866357)\n",
      "2023-08-08 16:47:32.077 | INFO     | __main__:get_response:186 - Min distance from Semantic results: 0.7428295612335205\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "from urllib.error import URLError\n",
    "import openai\n",
    "import os\n",
    "from collections import deque  \n",
    "from loguru import logger\n",
    "import sys\n",
    "import time\n",
    "# sys.path.append(\"..\")\n",
    "#from config import API_KEY\n",
    "import re\n",
    "import sys\n",
    "import traceback\n",
    "import json\n",
    "from time import time\n",
    "from typing import Callable\n",
    "#import flask\n",
    "# from flask import jsonify\n",
    "from loguru import logger\n",
    "from functools import wraps\n",
    "import pandas as pd\n",
    "\n",
    "# Langchain related\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from hr_ex import HrResult\n",
    "from common import (\n",
    "    GPT_ANSWER_THRESHOLD,\n",
    "    MIN_GPT_QUERY_LENGTH,\n",
    "    MIN_QUERY_LENGTH,\n",
    "    RANKER_NUM_RESULTS,\n",
    "    GPT_MODEL_PARAMS,\n",
    "    MODEL_FOLDER,\n",
    "    GPT_ANSWER_PREFIX,\n",
    ")\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = 'e1ad721cd0fc4e5a89a8c67d1ce6e75d'\n",
    "# os.environ['GPT_API_KEY'] = 'e1ad721cd0fc4e5a89a8c67d1ce6e75d'\n",
    "\n",
    "logger.remove()\n",
    "logger.add(sys.stderr, level=\"DEBUG\")\n",
    "\n",
    "################################# Langchain#####################################\n",
    "# If the GPT api key is not set, then fallback to just the semantic search\n",
    "# if os.getenv(\"GPT_API_KEY\") is None:\n",
    "#     logger.info(\"No GPT3 key, disabling this feature\")\n",
    "#     GPT_STATUS = False\n",
    "# else:\n",
    "#     logger.info(\"GPT3 API Key found\")\n",
    "#     os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"GPT_API_KEY\")\n",
    "#     GPT_STATUS = True\n",
    "GPT_STATUS = True\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set up OpenAI credentials\n",
    "#openai.api_base = \"https://siaec-data-gpt.openai.azure.com/\"\n",
    "#openai.api_type = 'azure'\n",
    "#openai.api_version = \"2023-03-15-preview\"\n",
    "#deployment_name = \"gpt-35-turbo\"\n",
    "#openai.api_key = API_KEY\n",
    "\n",
    "# st.set_page_config(page_title=\"Joey, the HR assistant! ðŸŽ­\", page_icon=\"ðŸŽ­\")\n",
    "# html_temp = \"\"\"\n",
    "# <div style=\"background-color:brown;padding:10px\">\n",
    "# <h2 style=\"color:white;text-align:center;\">Joey, the HR assistant! </h2>\n",
    "# </div>\n",
    "# \"\"\"\n",
    "# st.markdown(html_temp,unsafe_allow_html=True)\n",
    "# st.sidebar.header(\"Joey, the HR assistant!\")\n",
    "\n",
    "\n",
    "# if \"authenticated\" not in st.session_state:\n",
    "#     st.session_state.authenticated = True#False\n",
    "# if \"w2k_hash\" not in st.session_state:\n",
    "#     st.session_state.w2k_hash = \"\"\n",
    "# if \"w2k\" not in st.session_state:\n",
    "#     st.session_state.w2k = \"\"\n",
    "\n",
    "# if \"history\" not in st.session_state:\n",
    "#     st.session_state.history = []\n",
    "#     st.session_state.history.append(\n",
    "#         {\"role\": \"system\", \"content\": \"I'm HR AI assistant. How can i help you today?\"}\n",
    "#     )\n",
    "\n",
    "\n",
    "\n",
    "class Chatbot:\n",
    "    def __init__(self) -> None:\n",
    "        self.embeddings = HuggingFaceEmbeddings()\n",
    "        logger.info(f\"Embedding model = {self.embeddings.model_name}\")\n",
    "\n",
    "        logger.info(\"Reading in vectorstore DB from training script\")\n",
    "        self.db = FAISS.load_local(MODEL_FOLDER / \"faiss_index\", self.embeddings)\n",
    "\n",
    "        # 13 Jun 23: Change model to GPT3.5 on our Azure Openai resource\n",
    "        model = AzureChatOpenAI(\n",
    "            openai_api_base=GPT_MODEL_PARAMS[\"api_base\"],\n",
    "            openai_api_version=GPT_MODEL_PARAMS[\"api_version\"],\n",
    "            deployment_name=GPT_MODEL_PARAMS[\"deployment_name\"],\n",
    "            openai_api_key=\"e1ad721cd0fc4e5a89a8c67d1ce6e75d\",#os.getenv(\"GPT_API_KEY\"),\n",
    "            openai_api_type=GPT_MODEL_PARAMS[\"api_type\"],\n",
    "        )\n",
    "        self.chain = load_qa_with_sources_chain(\n",
    "            model,\n",
    "            chain_type=\"stuff\",\n",
    "        )\n",
    "\n",
    "    def get_response(\n",
    "        self,\n",
    "        query: str,\n",
    "        # hr_grade: str,\n",
    "        database_name: str = \"joey\",\n",
    "        query_gpt: bool = GPT_STATUS,\n",
    "    ) -> dict:\n",
    "        \"\"\"Executes the query and returns the results in the expected format to return\n",
    "        as a response to the user.\n",
    "\n",
    "        :param query: User query\n",
    "        :type query: str\n",
    "        :param hr_grade: User's hr grade, to be passed in the request\n",
    "        :type hr_grade: str\n",
    "        :param database_name: Currently not used; meant to enable querying from multiple vecdbs,\n",
    "        defaults to \"joey\"\n",
    "        :type database_name: str, optional\n",
    "        :param query_gpt: Flag to denote if GPT should be called, defaults to GPT_STATUS\n",
    "        :type query_gpt: bool, optional\n",
    "        :return: A dict of dicts containing the results. Top level key is the answer index,\n",
    "        inner keys are ['Question','Answer','Image','SimScore']\n",
    "        :rtype: dict\n",
    "        \"\"\"\n",
    "\n",
    "        # 28Mar23: Prepare the query\n",
    "        orig_query = query  # make a copy to display in the final output\n",
    "        query = pre_process(query)\n",
    "        logger.info(f\"Modified query: {query}\")\n",
    "\n",
    "        # Semantic similarity results. Outputs are [(doc,score)]\n",
    "        #! distance is returned, not similarity scores\n",
    "        raw_results = self.db.similarity_search_with_score(query, k=RANKER_NUM_RESULTS)\n",
    "\n",
    "        #! Process the results\n",
    "        #! To create the final answer, the best score from the semantic search is\n",
    "        #! extracted; if it's below a threshold, then GPT is called to generate the answer\n",
    "        #! otherwise, the semantic answers are returned. To prepare for this possibility, the\n",
    "        #! semantic answers are also formatted.\n",
    "        docs = []  # To pass to the chain as the context\n",
    "        results = {}  # Formatted results as a dict for potential display\n",
    "        ans_idx = 0  # Initial index for the formatted results\n",
    "        min_distance = 999  # To store the min distance found among the results\n",
    "        all_semantic_questions = (\n",
    "            \"\"  # To keep the questions only. Concatenated as a single string\n",
    "        )\n",
    "        top_answer = \"\"  # To keep the top answer only\n",
    "        for i, raw_result in enumerate(raw_results):\n",
    "            logger.debug(raw_result)\n",
    "\n",
    "            # Get the question,answer and score from the db entries\n",
    "            doc, score = raw_result\n",
    "            (q, a) = doc.page_content.split(\"##\")\n",
    "\n",
    "            if i == 0:\n",
    "                top_answer = q.strip() + \"\\n\" + a.strip() + \"\\n\"\n",
    "            else:\n",
    "                # Keep only the questions\n",
    "                all_semantic_questions += \"- \" + q.strip() + \"\\n\"\n",
    "\n",
    "            docs.append(doc)\n",
    "            results[\"Q\" + str(ans_idx)] = {\n",
    "                \"QUESTION\": q.strip(),\n",
    "                \"ANSWER\": a.strip(),\n",
    "                \"REFER\": doc.metadata[\"REFER\"],\n",
    "                \"IMAGE\": doc.metadata[\"IMAGE\"],\n",
    "                \"URL\": doc.metadata[\"URL\"],\n",
    "                \"SimScore\": score,\n",
    "            }\n",
    "            min_distance = min([min_distance, score])\n",
    "\n",
    "            ans_idx += 1\n",
    "        logger.info(f\"Min distance from Semantic results: {min_distance}\")\n",
    "\n",
    "\n",
    "        #! next blocks are only executed if gpt should be called which are based\n",
    "        #! on the min_distance from the semantic search and the query_gpt flag\n",
    "        #! The output can be 2 types: a) gpt is able to answer the query, b) gpt\n",
    "        #! does not know how to answer the query. In the former case, the gpt\n",
    "        #! response is formatted to extract the answer and the sources (where the\n",
    "        #! text comes from the vecdb). In the latter case, a default answer is given\n",
    "        #! whereby the top answer from the semantic search is returned along with\n",
    "        #! a list of other possible questions to try (comes from the questions in\n",
    "        #! the semantic search)\n",
    "        gpt_answer = GPT_ANSWER_PREFIX\n",
    "        # Call GPT only if the best answer from the semantic search is below threshold:\n",
    "        # i.e. there is a potential answer to the semantic answers\n",
    "        if min_distance <= GPT_ANSWER_THRESHOLD and query_gpt:\n",
    "            # Run the chain for the query\n",
    "            gpt_result = self.chain(\n",
    "                {\n",
    "                    \"input_documents\": docs,\n",
    "                    \"question\": query,\n",
    "                },\n",
    "                return_only_outputs=True,\n",
    "            )\n",
    "            logger.debug(gpt_result)\n",
    "\n",
    "            gpt_answer_text = gpt_result[\"output_text\"]\n",
    "\n",
    "            # gpt_answer_flag is used to indicate if the answer is a valid answer\n",
    "            # if false, a default answer is returned to indicate that the answer\n",
    "            # cannot be found.\n",
    "            if re.search(r\"I don't know\", gpt_answer_text):\n",
    "                gpt_answer_flag = False\n",
    "            else:\n",
    "                if re.search(\"(?=Source|Sources|SOURCE|SOURCES)\", gpt_answer_text):\n",
    "                    gpt_answer_flag = True\n",
    "                    parts = re.split(\n",
    "                        \"(?=Source|Sources|SOURCE|SOURCES)\", gpt_answer_text\n",
    "                    )\n",
    "\n",
    "                    gpt_answer += parts[0].strip() + \"\\n\"\n",
    "                    logger.info(f\"source_part: {parts[1]}\")\n",
    "                    sources = [x for x in re.findall(r\"S\\d+\", parts[1].strip())]\n",
    "                    sources = list(set(sources))\n",
    "                    logger.debug(f\"SOURCES: {sources}\")\n",
    "\n",
    "                    gpt_answer += f\"\\n{len(sources)} SOURCES:\\n\"\n",
    "\n",
    "                    for idx, source in enumerate(sources):\n",
    "                        for doc in docs:\n",
    "                            if source == doc.metadata[\"source\"]:\n",
    "                                (q, a) = doc.page_content.split(\"##\")\n",
    "                                gpt_answer += \"\\n\"\n",
    "                                gpt_answer += f\"[{idx+1}] \" + q.strip() + \"\\n\"\n",
    "                                gpt_answer += a.strip() + \"\\n\"\n",
    "                                gpt_answer = re.sub(r\"\\(S\\d+\\)\", \"\", gpt_answer)\n",
    "                else:\n",
    "                    gpt_answer_flag = False\n",
    "        else:\n",
    "            gpt_answer_flag = False\n",
    "\n",
    "        # If an answer is not found, modify the output by returning the top answer\n",
    "        # (in case it happens to be correct) and a list of possible questions coming\n",
    "        # from the semantic search\n",
    "        if not gpt_answer_flag and query_gpt:\n",
    "            gpt_answer += (\n",
    "                \"Unfortunately, I do not know how to answer your question directly.\\n\"\n",
    "            )\n",
    "            gpt_answer += \"This is the closest answer from the KnowledgeBase:\\n\\n\"\n",
    "            gpt_answer += top_answer\n",
    "            gpt_answer += (\n",
    "                \"\\nAlternatively, you may want to try one of these questions instead:\\n\"\n",
    "            )\n",
    "            gpt_answer += all_semantic_questions\n",
    "            gpt_answer += \"\\nOtherwise, please log a ticket with Joey.\"\n",
    "\n",
    "        if query_gpt:\n",
    "            logger.info(f\"[FINAL ANSWER]\\n{gpt_answer}\")\n",
    "\n",
    "            results = {}\n",
    "            results[\"Q0\"] = {\n",
    "                \"QUESTION\": orig_query.strip(),\n",
    "                \"ANSWER\": gpt_answer,\n",
    "                \"REFER\": \"EMPTY\",\n",
    "                \"IMAGE\": \"EMPTY\",\n",
    "                \"URL\": \"\",\n",
    "                \"SimScore\": 1,\n",
    "            }\n",
    "\n",
    "        return print(results[\"Q0\"][\"ANSWER\"])#hr_extra_result.hr_response(\n",
    "            # pd.DataFrame.from_dict(results, orient=\"index\")\n",
    "        # )\n",
    "\n",
    "\n",
    "############################ HELPER FUNCTIONS ##################################\n",
    "def send_error_response(error_message: str, container_code: str, error_code: str):\n",
    "    return error_message,container_code,error_code\n",
    "\n",
    "\n",
    "def measure(func: Callable) -> Callable:\n",
    "    \"\"\"This is a decorator function that measures the execution time of the function\n",
    "    it decorates.\n",
    "\n",
    "    :param func: A function to measure\n",
    "    :type func: Callable\n",
    "    :return: A wrapped function\n",
    "    :rtype: Callable\n",
    "    \"\"\"\n",
    "\n",
    "#    @wraps(func)\n",
    "# def _time_it(*args, **kwargs):\n",
    "#     start = int(round(time() * 1000000))\n",
    "#     try:\n",
    "#         return func(*args, **kwargs)\n",
    "#     finally:\n",
    "#         end_ = int(round(time() * 1000000)) - start\n",
    "#         logger.info(f\"Total execution/response time: {end_ if end_ > 0 else 0} us \")\n",
    "\n",
    "#     return _time_it\n",
    "\n",
    "\n",
    "def count_query_length(text: str) -> int:\n",
    "    \"\"\"Count the number of tokens in the query\n",
    "\n",
    "    :param text: Query\n",
    "    :type text: str\n",
    "    :return: Number of tokens\n",
    "    :rtype: int\n",
    "    \"\"\"\n",
    "\n",
    "    parts = \" \".join(re.compile(r\"\\W+\", re.UNICODE).split(text))\n",
    "    zz = re.sub(\"[^a-zA-Z]+\", \" \", parts).strip()\n",
    "    text_length = len(zz.split(\" \"))\n",
    "    return text_length\n",
    "\n",
    "\n",
    "def pre_process(query: str) -> str:\n",
    "    \"\"\"Modify the query before running the search\n",
    "\n",
    "    :param query: The user query\n",
    "    :type query: str\n",
    "    :return: Modified query\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    if re.search(r\"(travel\\s)?subload\", query):\n",
    "        query = re.sub(\n",
    "            r\"(travel\\s)?subload\", \"leisure travel subject-to-load-basis\", query\n",
    "        )\n",
    "\n",
    "    return query\n",
    "\n",
    "\n",
    "################################# SETUP ########################################\n",
    "logger.info(\"Start of predictor\")\n",
    "logger.info(\"Creating the chatbot database using previously saved document store\")\n",
    "\n",
    "logger.info(\"Creating the chatbot object\")\n",
    "################################# FLASK ########################################\n",
    "# The flask app for serving predictions\n",
    "#app = flask.Flask(__name__)\n",
    "\n",
    "\n",
    "#@app.route(\"/ping\", methods=[\"GET\"])\n",
    "def ping():\n",
    "    status = 200\n",
    "    return \"Ping Successful.\" #flask.Response(\n",
    "#        response=\"Ping Successful.\", status=status, mimetype=\"application/json\"\n",
    "#    )\n",
    "\n",
    "\n",
    "#@app.route(\"/invocations\", methods=[\"POST\"])\n",
    "#@measure\n",
    "def transformation():\n",
    "    try:\n",
    "        logger.info(\"*\" * 20)\n",
    "        logger.info(\"Start /invocations\")\n",
    "\n",
    "        # if flask.request.is_json:\n",
    "        #     data = flask.request.get_json()\n",
    "        #     logger.debug(f\"Json Query packet: {data}\")\n",
    "\n",
    "            # Check required keys in request\n",
    "        # if \"Query\" not in data.keys():\n",
    "        #     return send_error_response(\n",
    "        #         \"Please specify 'Query' in the request.\", 4000, 400\n",
    "        #     )\n",
    "\n",
    "        # if \"Source\" not in data.keys():\n",
    "        #     return send_error_response(\n",
    "        #         \"Please specify 'Source' in the request.\", 4000, 400\n",
    "        #     )\n",
    "\n",
    "        # Return error response if query is too short\n",
    "        query_length = count_query_length(query)\n",
    "        logger.info(f\"Query: {query}\\nQuery length: {query_length}\")\n",
    "        if query_length < MIN_QUERY_LENGTH:\n",
    "            logger.info(\"User query too short. Returning error\")\n",
    "\n",
    "            return send_error_response(\n",
    "                \"Query is too short, please input at least 2 words for the query.\",\n",
    "                4016,\n",
    "                416,\n",
    "            )\n",
    "\n",
    "        # The openai_query flag is to indicate to use GPT answer where possible\n",
    "        # use_openai = False\n",
    "        # use_openai = bool(data[\"openai_query\"])\n",
    "        # if \"openai_query\" in data and str(data[\"openai_query\"]).lower() in [\n",
    "        #     \"true\",\n",
    "        #     \"1\",\n",
    "        #     \"y\",\n",
    "        #     \"yes\",\n",
    "        # ]:\n",
    "        use_openai = True\n",
    "        logger.info(f\"Use Generative Answer = {use_openai}\")\n",
    "\n",
    "        if query_length < MIN_GPT_QUERY_LENGTH:\n",
    "            \"\"\"\n",
    "            If the query is too short, the Info Retrieval search could match\n",
    "            easily to many docs in the KB because the intent of the query\n",
    "            is not clear. This in turn will create a Context with potentially\n",
    "            irrelevant info. When sent to GPT, a low quality answer could\n",
    "            result.\n",
    "            The parameter MIN_GPT_QUERY_LENGTH is set in common.py is currently\n",
    "            from heuristics. Can be adjusted higher if shorter queries still\n",
    "            gives poor GPT results.\n",
    "            \"\"\"\n",
    "            logger.info(\"Query is too short for GPT\")\n",
    "            use_openai = False  # overrides the request flag\n",
    "\n",
    "        result = kris_chat.get_response(\n",
    "            query,\n",
    "            database_name=\"Joey\",\n",
    "            query_gpt=use_openai,\n",
    "        )\n",
    "\n",
    "        logger.debug(json.dumps(result, indent=4))\n",
    "\n",
    "        return print(result)#flask.Response(\n",
    "#                response=json.dumps(result), status=200, mimetype=\"application/json\"\n",
    "#            )\n",
    "\n",
    "        # else:\n",
    "        #     return send_error_response(\"Only JSON format is supported.\", 4000, 400)\n",
    "    except Exception:\n",
    "        traceback.print_exc()\n",
    "        logger.exception(\"Error is transformation() function\")\n",
    "        return print(\"Sorry not sure if I fully understand- please elaborate your question.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# if st.session_state.authenticated is True:\n",
    "#     # auth=st.session_state.authenticated\n",
    "#     st.write(\"\"\"When asking a question, be as detailed as possible. Ex: if you want to ask about medical leaves, ask: Am I eligible to apply for under medical leave? If your question is not very specific, Joey will share the closest questions with answers that match your query.\"\"\")\n",
    "\n",
    "    # st.write(\"\"\"Please input your prompt here, if your prompt is quite long, can use the handle on bottom right to adjust the input box height.\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "    # with st.form(\"gpt_form\"):\n",
    "    #     query = st.text_area(\"Please input your query here.\",\n",
    "    #         \"\")\n",
    "    #     submit_button = st.form_submit_button(\"Send\", type='primary')\n",
    "    #     if submit_button and query:\n",
    "    #         start_time = time.time()\n",
    "    #         st.session_state.history.append({\"role\": \"user\", \"content\": query})\n",
    "query='How many days of medical leave I can take for critical illness?'\n",
    "kris_chat = Chatbot()\n",
    "res=transformation()\n",
    "\n",
    "# used to modify the output based on user grade. Currently this is a passthrough\n",
    "# hr_extra_result = HrResult()\n",
    "\n",
    "#             try:\n",
    "#                 response = openai.ChatCompletion.create(engine=deployment_name,messages=st.session_state.history,temperature=0)\n",
    "#                 logger.info(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "#                 result=\"\"\n",
    "#                 result = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "#                 st.success('{}'.format(result))\n",
    "#                 end_time = time.time()\n",
    "#                 elapsed_time = end_time - start_time\n",
    "\n",
    "#                 # Add assistant response to the conversation history and limit to 5 messages\n",
    "#                 st.session_state.history.append(\n",
    "#                     {\"role\": \"assistant\", \"content\": result}\n",
    "#                 )\n",
    "#                 if len(st.session_state.history) > 10:\n",
    "#                     st.session_state.history.pop(0)\n",
    "#                     st.session_state.history.pop(0)\n",
    "#                 # token count\n",
    "#                 prompt_token_count = int(response[\"usage\"][\"prompt_tokens\"])\n",
    "#                 answer_token_count = int(response[\"usage\"][\"completion_tokens\"])\n",
    "#                 # how many seconds used for the whole query\n",
    "#                 query_time = round(elapsed_time, 2)\n",
    "\n",
    "#                 # logger.info(\n",
    "#                 #     f\"{st.session_state.w2k_hash}|{AZURE_ENGINE}|{user_input}|{assistant_response}|Prompt_token_count:{str(prompt_token_count)}|Answer_token_count:{(answer_token_count)}|Query_time:{(query_time)}\"\n",
    "#                 # )\n",
    "#                 # for saa gamification\n",
    "#                 logger.info(\n",
    "#                     f\"{st.session_state.w2k}|{deployment_name}|{defect_text}|{result}|Prompt_token_count:{str(prompt_token_count)}|Answer_token_count:{(answer_token_count)}|Query_time:{(query_time)}\"\n",
    "#                 )\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 logger.error(e)\n",
    "#                 st.warning(\n",
    "#                     \"We apologize for the inconvenience. The Microsoft service is currently beyond capacity. We have sent a message to our Data team to investigate and we hope to have it resolved soon. Please check back later for updates. Thank you for your patience.\"\n",
    "#                 )\n",
    "#     acco = st.expander(\"Conversation history\", expanded=True)\n",
    "#     for message in st.session_state.history:\n",
    "#         acco.write(f'{message[\"role\"].capitalize()}: {message[\"content\"]}')\n",
    "\n",
    "\n",
    "#     if st.button(\"About\"):\n",
    "# #        st.text(\"Lets Learn\")\n",
    "#         st.text(\"Please reach out to sahil_sharma for feedback.\")\n",
    "# else: st.warning(\"Please login from the main page first\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6cffc3d1-40ce-48f8-a733-e9abbf5db4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c7966d-955f-44ce-a766-bd22e84bbc82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3b6ea7-16dd-4eeb-bb60-ead45a701fed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be381f59-df82-4929-82a2-921a035880b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bebe8c7f-9d19-42c5-8173-7b953be0052a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-07 23:31:31.676 INFO    sentence_transformers.SentenceTransformer: Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "2023-08-07 23:31:32.341 INFO    sentence_transformers.SentenceTransformer: Use pytorch device: cpu\n",
      "2023-08-07 23:31:32.342 | INFO     | __main__:__init__:97 - Embedding model = sentence-transformers/all-mpnet-base-v2\n",
      "2023-08-07 23:31:32.342 | INFO     | __main__:__init__:99 - Reading in vectorstore DB from training script\n",
      "2023-08-07 23:31:32.349 | INFO     | __main__:get_response:138 - Modified query: What if I have used up my annual leave?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bai\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad730636fab44abe83dd263f4ba36a1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-07 23:31:32.415 | DEBUG    | __main__:get_response:159 - (Document(page_content='Annual Leave: Utilisation ## \\u200bUtilise on one-day or half-day basis within the calendar year in which it is earned; unutilised leave can be carried forward into the next calendar year. As a guide, such carrying forward of leave should not go beyond 31 March of the following year. Any leave unutilised by the end of the following year (2 yearsâ€™ validity) will automatically lapse. Taking of advance leave is strongly discouraged. Where required, you should apply for no-pay leave.', metadata={'REFER': 'EMPTY', 'IMAGE': 'EMPTY', 'URL': nan, 'source': 'S505'}), 0.5175082)\n",
      "2023-08-07 23:31:32.415 | DEBUG    | __main__:get_response:159 - (Document(page_content='Annual Leave: Eligibility & Entitlement ## You are eligible for annual leave after 3 months of service with the Company. Your entitlement is as stated in your employment contract. You may also refer to the Eligibility & Entitlement tab at this <a href=\"awb://sia.sharepoint.com/sites/Intranet/SitePages/HR-Journey3.aspx?Journey=J02&Topic1=J02-T02&Topic2=J02-T02-04&Topic3=J02-T02-04-01\">page</a> for the annual leave entitlement for the respective staff groups. ', metadata={'REFER': 'EMPTY', 'IMAGE': 'EMPTY', 'URL': nan, 'source': 'S504'}), 0.6652296)\n",
      "2023-08-07 23:31:32.416 | DEBUG    | __main__:get_response:159 - (Document(page_content='Annual Leave: Application ## Application, changes and cancellations can be made via myHR / 1SQ > Leave (select time type as Annual Leave). Approval is required before commencement of absences.', metadata={'REFER': 'EMPTY', 'IMAGE': 'EMPTY', 'URL': nan, 'source': 'S510'}), 0.70539165)\n",
      "2023-08-07 23:31:32.416 | DEBUG    | __main__:get_response:159 - (Document(page_content='Can I carry forward my annual leave to the following year? ## As a good practice, you should utilise your annual leave by 31 December of the current year. \\nIf you are unable to finish utilising your annual leave within the calendar year due to operational reasons, you may discuss with your department head for your leave to be cleared by 31 March the following year.', metadata={'REFER': 'EMPTY', 'IMAGE': 'EMPTY', 'URL': nan, 'source': 'S9'}), 0.72632277)\n",
      "2023-08-07 23:31:32.416 | DEBUG    | __main__:get_response:159 - (Document(page_content='Annual Leave: Retired & re-employed employee ## You should utilise all earned annual leave upon retirement and before the start of the re-employment contract, unless there are exceptional circumstances to warrant otherwise. Annual leave entitlement offered as per your re-employment contract must be utilised within the contract year.\\u200b', metadata={'REFER': 'EMPTY', 'IMAGE': 'EMPTY', 'URL': nan, 'source': 'S509'}), 0.74078906)\n",
      "2023-08-07 23:31:32.417 | INFO     | __main__:get_response:183 - Min distance from Semantic results: 0.5175082087516785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baibai\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "from urllib.error import URLError\n",
    "import openai\n",
    "import os\n",
    "from collections import deque  \n",
    "from loguru import logger\n",
    "import sys\n",
    "import time\n",
    "# sys.path.append(\"..\")\n",
    "#from config import API_KEY\n",
    "import re\n",
    "import sys\n",
    "import traceback\n",
    "import json\n",
    "from time import time\n",
    "from typing import Callable\n",
    "#import flask\n",
    "# from flask import jsonify\n",
    "from loguru import logger\n",
    "from functools import wraps\n",
    "import pandas as pd\n",
    "\n",
    "# Langchain related\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from hr_ex import HrResult\n",
    "from common import (\n",
    "    GPT_ANSWER_THRESHOLD,\n",
    "    MIN_GPT_QUERY_LENGTH,\n",
    "    MIN_QUERY_LENGTH,\n",
    "    RANKER_NUM_RESULTS,\n",
    "    GPT_MODEL_PARAMS,\n",
    "    MODEL_FOLDER,\n",
    "    GPT_ANSWER_PREFIX,\n",
    ")\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = 'e1ad721cd0fc4e5a89a8c67d1ce6e75d'\n",
    "# os.environ['GPT_API_KEY'] = 'e1ad721cd0fc4e5a89a8c67d1ce6e75d'\n",
    "\n",
    "logger.remove()\n",
    "logger.add(sys.stderr, level=\"DEBUG\")\n",
    "\n",
    "################################# Langchain#####################################\n",
    "# If the GPT api key is not set, then fallback to just the semantic search\n",
    "# if os.getenv(\"GPT_API_KEY\") is None:\n",
    "#     logger.info(\"No GPT3 key, disabling this feature\")\n",
    "#     GPT_STATUS = False\n",
    "# else:\n",
    "#     logger.info(\"GPT3 API Key found\")\n",
    "#     os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"GPT_API_KEY\")\n",
    "#     GPT_STATUS = True\n",
    "GPT_STATUS = True\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set up OpenAI credentials\n",
    "#openai.api_base = \"https://siaec-data-gpt.openai.azure.com/\"\n",
    "#openai.api_type = 'azure'\n",
    "#openai.api_version = \"2023-03-15-preview\"\n",
    "#deployment_name = \"gpt-35-turbo\"\n",
    "#openai.api_key = API_KEY\n",
    "\n",
    "# st.set_page_config(page_title=\"Joey, the HR assistant! ðŸŽ­\", page_icon=\"ðŸŽ­\")\n",
    "# html_temp = \"\"\"\n",
    "# <div style=\"background-color:brown;padding:10px\">\n",
    "# <h2 style=\"color:white;text-align:center;\">Joey, the HR assistant! </h2>\n",
    "# </div>\n",
    "# \"\"\"\n",
    "# st.markdown(html_temp,unsafe_allow_html=True)\n",
    "# st.sidebar.header(\"Joey, the HR assistant!\")\n",
    "\n",
    "\n",
    "# if \"authenticated\" not in st.session_state:\n",
    "#     st.session_state.authenticated = True#False\n",
    "# if \"w2k_hash\" not in st.session_state:\n",
    "#     st.session_state.w2k_hash = \"\"\n",
    "# if \"w2k\" not in st.session_state:\n",
    "#     st.session_state.w2k = \"\"\n",
    "\n",
    "# if \"history\" not in st.session_state:\n",
    "#     st.session_state.history = []\n",
    "#     st.session_state.history.append(\n",
    "#         {\"role\": \"system\", \"content\": \"I'm HR AI assistant. How can i help you today?\"}\n",
    "#     )\n",
    "\n",
    "\n",
    "\n",
    "class Chatbot:\n",
    "    def __init__(self) -> None:\n",
    "        self.embeddings = HuggingFaceEmbeddings()\n",
    "        logger.info(f\"Embedding model = {self.embeddings.model_name}\")\n",
    "\n",
    "        logger.info(\"Reading in vectorstore DB from training script\")\n",
    "        self.db = FAISS.load_local(MODEL_FOLDER / \"faiss_index\", self.embeddings)\n",
    "\n",
    "        # 13 Jun 23: Change model to GPT3.5 on our Azure Openai resource\n",
    "        model = AzureChatOpenAI(\n",
    "            openai_api_base=GPT_MODEL_PARAMS[\"api_base\"],\n",
    "            openai_api_version=GPT_MODEL_PARAMS[\"api_version\"],\n",
    "            deployment_name=GPT_MODEL_PARAMS[\"deployment_name\"],\n",
    "            openai_api_key=\"e1ad721cd0fc4e5a89a8c67d1ce6e75d\",#os.getenv(\"GPT_API_KEY\"),\n",
    "            openai_api_type=GPT_MODEL_PARAMS[\"api_type\"],\n",
    "        )\n",
    "        self.chain = load_qa_with_sources_chain(\n",
    "            model,\n",
    "            chain_type=\"stuff\",\n",
    "        )\n",
    "\n",
    "    def get_response(\n",
    "        self,\n",
    "        query: str\n",
    "    ) -> dict:\n",
    "        \"\"\"Executes the query and returns the results in the expected format to return\n",
    "        as a response to the user.\n",
    "\n",
    "        :param query: User query\n",
    "        :type query: str\n",
    "        :param hr_grade: User's hr grade, to be passed in the request\n",
    "        :type hr_grade: str\n",
    "        :param database_name: Currently not used; meant to enable querying from multiple vecdbs,\n",
    "        defaults to \"joey\"\n",
    "        :type database_name: str, optional\n",
    "        :param query_gpt: Flag to denote if GPT should be called, defaults to GPT_STATUS\n",
    "        :type query_gpt: bool, optional\n",
    "        :return: A dict of dicts containing the results. Top level key is the answer index,\n",
    "        inner keys are ['Question','Answer','Image','SimScore']\n",
    "        :rtype: dict\n",
    "        \"\"\"\n",
    "\n",
    "        # 28Mar23: Prepare the query\n",
    "        orig_query = query  # make a copy to display in the final output\n",
    "        logger.info(f\"Modified query: {query}\")\n",
    "\n",
    "        # Semantic similarity results. Outputs are [(doc,score)]\n",
    "        #! distance is returned, not similarity scores\n",
    "        raw_results = self.db.similarity_search_with_score(query, k=RANKER_NUM_RESULTS)\n",
    "        print(\"baibai\")\n",
    "\n",
    "        #! Process the results\n",
    "        #! To create the final answer, the best score from the semantic search is\n",
    "        #! extracted; if it's below a threshold, then GPT is called to generate the answer\n",
    "        #! otherwise, the semantic answers are returned. To prepare for this possibility, the\n",
    "        #! semantic answers are also formatted.\n",
    "        docs = []  # To pass to the chain as the context\n",
    "        results = {}  # Formatted results as a dict for potential display\n",
    "        ans_idx = 0  # Initial index for the formatted results\n",
    "        min_distance = 999  # To store the min distance found among the results\n",
    "        all_semantic_questions = (\n",
    "            \"\"  # To keep the questions only. Concatenated as a single string\n",
    "        )\n",
    "        top_answer = \"\"  # To keep the top answer only\n",
    "        for i, raw_result in enumerate(raw_results):\n",
    "            logger.debug(raw_result)\n",
    "\n",
    "            # Get the question,answer and score from the db entries\n",
    "            doc, score = raw_result\n",
    "            (q, a) = doc.page_content.split(\"##\")\n",
    "\n",
    "            if i == 0:\n",
    "                top_answer = q.strip() + \"\\n\" + a.strip() + \"\\n\"\n",
    "            else:\n",
    "                # Keep only the questions\n",
    "                all_semantic_questions += \"- \" + q.strip() + \"\\n\"\n",
    "\n",
    "            docs.append(doc)\n",
    "            results[\"Q\" + str(ans_idx)] = {\n",
    "                \"QUESTION\": q.strip(),\n",
    "                \"ANSWER\": a.strip(),\n",
    "                \"REFER\": doc.metadata[\"REFER\"],\n",
    "                \"IMAGE\": doc.metadata[\"IMAGE\"],\n",
    "                \"URL\": doc.metadata[\"URL\"],\n",
    "                \"SimScore\": score,\n",
    "            }\n",
    "            min_distance = min([min_distance, score])\n",
    "\n",
    "            ans_idx += 1\n",
    "        logger.info(f\"Min distance from Semantic results: {min_distance}\")\n",
    "        return results#[\"Q0\"][\"ANSWER\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def transformation():\n",
    "    # try:\n",
    "    print(\"bai\")\n",
    "    result = kris_chat.get_response(query)\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "query='What if I have used up my annual leave?'\n",
    "kris_chat = Chatbot()\n",
    "res=transformation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03dd3956-e06f-4667-a675-3f44764a32ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Q0': {'QUESTION': 'Annual Leave: Utilisation',\n",
       "  'ANSWER': '\\u200bUtilise on one-day or half-day basis within the calendar year in which it is earned; unutilised leave can be carried forward into the next calendar year. As a guide, such carrying forward of leave should not go beyond 31 March of the following year. Any leave unutilised by the end of the following year (2 yearsâ€™ validity) will automatically lapse. Taking of advance leave is strongly discouraged. Where required, you should apply for no-pay leave.',\n",
       "  'REFER': 'EMPTY',\n",
       "  'IMAGE': 'EMPTY',\n",
       "  'URL': nan,\n",
       "  'SimScore': 0.5175082},\n",
       " 'Q1': {'QUESTION': 'Annual Leave: Eligibility & Entitlement',\n",
       "  'ANSWER': 'You are eligible for annual leave after 3 months of service with the Company. Your entitlement is as stated in your employment contract. You may also refer to the Eligibility & Entitlement tab at this <a href=\"awb://sia.sharepoint.com/sites/Intranet/SitePages/HR-Journey3.aspx?Journey=J02&Topic1=J02-T02&Topic2=J02-T02-04&Topic3=J02-T02-04-01\">page</a> for the annual leave entitlement for the respective staff groups.',\n",
       "  'REFER': 'EMPTY',\n",
       "  'IMAGE': 'EMPTY',\n",
       "  'URL': nan,\n",
       "  'SimScore': 0.6652296},\n",
       " 'Q2': {'QUESTION': 'Annual Leave: Application',\n",
       "  'ANSWER': 'Application, changes and cancellations can be made via myHR / 1SQ > Leave (select time type as Annual Leave). Approval is required before commencement of absences.',\n",
       "  'REFER': 'EMPTY',\n",
       "  'IMAGE': 'EMPTY',\n",
       "  'URL': nan,\n",
       "  'SimScore': 0.70539165},\n",
       " 'Q3': {'QUESTION': 'Can I carry forward my annual leave to the following year?',\n",
       "  'ANSWER': 'As a good practice, you should utilise your annual leave by 31 December of the current year. \\nIf you are unable to finish utilising your annual leave within the calendar year due to operational reasons, you may discuss with your department head for your leave to be cleared by 31 March the following year.',\n",
       "  'REFER': 'EMPTY',\n",
       "  'IMAGE': 'EMPTY',\n",
       "  'URL': nan,\n",
       "  'SimScore': 0.72632277},\n",
       " 'Q4': {'QUESTION': 'Annual Leave: Retired & re-employed employee',\n",
       "  'ANSWER': 'You should utilise all earned annual leave upon retirement and before the start of the re-employment contract, unless there are exceptional circumstances to warrant otherwise. Annual leave entitlement offered as per your re-employment contract must be utilised within the contract year.\\u200b',\n",
       "  'REFER': 'EMPTY',\n",
       "  'IMAGE': 'EMPTY',\n",
       "  'URL': nan,\n",
       "  'SimScore': 0.74078906}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2583de-291a-4dc7-afba-8e078a6998fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
